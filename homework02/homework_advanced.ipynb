{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NqtXrZApBkM4"
   },
   "source": [
    "# Optimizing training and inference\n",
    "\n",
    "In this notebook, we will discuss different ways to reduce memory and compute usage during training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NEt8wg4JCQdm"
   },
   "source": [
    "## Prepare training script\n",
    "\n",
    "When training large models, it is usually a best practice not to use Jupyter notebooks, but run a **separate script** for training which could have command-line flags for various hyperparameters and training modes. This is especially useful when you need to run multiple experiments simultaneously (e.g. on a cluster with task scheduler). Another advantage of this is that after training, the process will finish and free the resources for other users of a shared GPU.\n",
    "\n",
    "In this part, you will need to put all your code to train a model on Tiny ImageNet that you wrote for the previous task in `train.py`.\n",
    "\n",
    "You can then run your script from inside of this notebook like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6-TWiKq8H9yT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 2 took 45.768s\n",
      "  training loss (in-iteration): \t2.496664\n",
      "  validation accuracy: \t\t\t48.23 %\n",
      "Epoch 2 of 2 took 45.975s\n",
      "  training loss (in-iteration): \t2.453964\n",
      "  validation accuracy: \t\t\t46.06 %\n",
      "Final results:\n",
      "  test accuracy:\t\t42.25 %\n"
     ]
    }
   ],
   "source": [
    "! python3 train.py --batch_size 128 --epochs 2 --gpu_enabled \\\n",
    "                  --model_path model_state_dict_41.71.pcl \\\n",
    "                  --data_path tiny-imagenet-200 \\\n",
    "                  --model_module_path model.py \\\n",
    "                  --model_checkpoint_path model_checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** \n",
    "\n",
    "Write code for training with architecture from homework_part2\n",
    "\n",
    "**Requirements**\n",
    "* Optional arguments from command line such as batch size and number of epochs with built-in argparse\n",
    "* Modular structure - separate functions for creating data generator, building model and training \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tKPYZ3QLEqX8"
   },
   "source": [
    "## Profiling time\n",
    "\n",
    "For the next tasks, you need to add measurements to your training loop. You can use [`perf_counter`](https://docs.python.org/3/library/time.html#time.perf_counter) for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bSr-PyQNFkSC"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HMJMCGRKFYCc",
    "outputId": "571046a2-443b-465f-ce62-ddaf68b105d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication took 0.020 seconds\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(1000, 1000)\n",
    "y = np.random.randn(1000, 1000)\n",
    "\n",
    "start_counter = time.perf_counter()\n",
    "z = x @ y\n",
    "elapsed_time = time.perf_counter() - start_counter\n",
    "print(\"Matrix multiplication took {:.3f} seconds\".format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 2 took 45.448s\n",
      "  training loss (in-iteration): \t2.495216\n",
      "  validation accuracy: \t\t\t47.26 %\n",
      "\t\tForward pass took  2.290 seconds\n",
      "\t\tBackward pass took 1.755 seconds\n",
      "Epoch 2 of 2 took 45.807s\n",
      "  training loss (in-iteration): \t2.457833\n",
      "  validation accuracy: \t\t\t46.19 %\n",
      "\t\tForward pass took  2.262 seconds\n",
      "\t\tBackward pass took 1.715 seconds\n",
      "Final results:\n",
      "  test accuracy:\t\t41.47 %\n"
     ]
    }
   ],
   "source": [
    "! python3 train.py --batch_size 128 --epochs 2 --gpu_enabled \\\n",
    "                  --model_path model_state_dict_41.71.pcl \\\n",
    "                  --data_path tiny-imagenet-200 \\\n",
    "                  --model_module_path model.py \\\n",
    "                  --model_checkpoint_path model_checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FfhLeWjTGTpB"
   },
   "source": [
    "**Task**. You need to add the following measurements to your training script:\n",
    "* How much time a forward-backward pass takes for a single batch;\n",
    "* How much time an epoch takes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "khDOTn_SHaND"
   },
   "source": [
    "## Profiling memory usage\n",
    "\n",
    "**Task**. You need to measure the memory consumptions\n",
    "\n",
    "This section depends on whether you train on CPU or GPU.\n",
    "\n",
    "### If you train on CPU\n",
    "You can use GNU time to measure peak RAM usage of a script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98xvXSjUIDzl"
   },
   "outputs": [],
   "source": [
    "!/usr/bin/time -lp python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v1ES2Pc9IlH5"
   },
   "source": [
    "**Maximum resident set size**  will show you the peak RAM usage in bytes after the script finishes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**. \n",
    "Imports also require memory, do the correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kq5lY5CKJHX1"
   },
   "source": [
    "### If you train on GPU\n",
    "\n",
    "Use [`torch.cuda.max_memory_allocated()`](https://pytorch.org/docs/stable/cuda.html#torch.cuda.max_memory_allocated) at the end of your script to show the maximum amount of memory in bytes used by all tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fSQdauqLIkf1",
    "outputId": "8bcffc30-637d-461a-8f44-0e444a28caae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak memory usage by Pytorch tensors: 3815.58 Mb\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1000, 1000, 1000, device='cuda:0')\n",
    "print(\"Peak memory usage by Pytorch tensors: {:.2f} Mb\".format((torch.cuda.max_memory_allocated() / 1024 / 1024)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 2 took 45.652s\n",
      "  training loss (in-iteration): \t2.493698\n",
      "  validation accuracy: \t\t\t47.80 %\n",
      "\t\tForward pass took  2.287 seconds\n",
      "\t\tBackward pass took 1.732 seconds\n",
      "Epoch 2 of 2 took 45.916s\n",
      "  training loss (in-iteration): \t2.452110\n",
      "  validation accuracy: \t\t\t45.72 %\n",
      "\t\tForward pass took  2.271 seconds\n",
      "\t\tBackward pass took 1.729 seconds\n",
      "Final results:\n",
      "  test accuracy:\t\t41.35 %\n",
      "Peak memory usage by Pytorch tensors: 1660.91 Mb\n"
     ]
    }
   ],
   "source": [
    "! python3 train.py --batch_size 128 --epochs 2 --gpu_enabled \\\n",
    "                  --model_path model_state_dict_41.71.pcl \\\n",
    "                  --data_path tiny-imagenet-200 \\\n",
    "                  --model_module_path model.py \\\n",
    "                  --model_checkpoint_path model_checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3RWHxYKBUys"
   },
   "source": [
    "## Gradient based techniques\n",
    "\n",
    "Modern architectures can potentially consume lots and lots of memory even for minibatch of several objects. To handle such cases here we will discuss two simple techniques.\n",
    "\n",
    "### Gradient Checkpointing\n",
    "\n",
    "Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does not save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model.\n",
    "\n",
    "See [blogpost](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9) for kind introduction and different strategies or [article](https://arxiv.org/pdf/1604.06174.pdf) for not kind introduction.\n",
    "\n",
    "**Task**. Use [built-in checkpointing](https://pytorch.org/docs/stable/checkpoint.html), measure the difference in memory/compute \n",
    "\n",
    "**Requirements**. \n",
    "* Try several arrangements for checkpoints\n",
    "* Add the chekpointing as the optional flag into your script\n",
    "* Measure the difference in memory/compute between the different arrangements and baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 2 took 24.709s\n",
      "  training loss (in-iteration): \t2.426903\n",
      "  validation accuracy: \t\t\t50.22 %\n",
      "\t\tForward pass took  10.040 seconds\n",
      "\t\tBackward pass took 0.419 seconds\n",
      "Epoch 2 of 2 took 24.785s\n",
      "  training loss (in-iteration): \t2.365057\n",
      "  validation accuracy: \t\t\t48.75 %\n",
      "\t\tForward pass took  10.074 seconds\n",
      "\t\tBackward pass took 0.524 seconds\n",
      "Final results:\n",
      "  test accuracy:\t\t43.05 %\n",
      "Peak memory usage by Pytorch tensors: 1650.14 Mb\n"
     ]
    }
   ],
   "source": [
    "! python3 -W ignore train.py --batch_size 128 --epochs 2 --gpu_enabled --checkpoint \\\n",
    "                             --model_path model_state_dict_41.71.pcl \\\n",
    "                             --data_path tiny-imagenet-200 \\\n",
    "                             --model_module_path model.py \\\n",
    "                             --model_checkpoint_path model_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# память уменьшилась незначительно, но стало быстрее почти в 2 раза за итерацию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mjY8LR_GQbTV"
   },
   "source": [
    "### Accumulating gradient for large batches\n",
    "We can increase the effective batch size by simply accumulating gradients over multiple forward passes. Note that `loss.backward()` simply adds the computed gradient to `tensor.grad`, so we can call this method multiple times before actually taking an optimizer step. However, this approach might be a little tricky to combine with batch normalization. Do you see why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 2 took 42.368s\n",
      "  training loss (in-iteration): \t2.312247\n",
      "  validation accuracy: \t\t\t53.63 %\n",
      "\t\tForward pass took  1.855 seconds\n",
      "\t\tBackward pass took 1.853 seconds\n",
      "Saving new best model!\n",
      "Epoch 2 of 2 took 42.571s\n",
      "  training loss (in-iteration): \t2.205992\n",
      "  validation accuracy: \t\t\t53.48 %\n",
      "\t\tForward pass took  1.850 seconds\n",
      "\t\tBackward pass took 1.790 seconds\n",
      "Final results:\n",
      "  test accuracy:\t\t44.61 %\n",
      "Peak memory usage by Pytorch tensors: 1660.91 Mb\n"
     ]
    }
   ],
   "source": [
    "! python3 -W ignore train.py --batch_size 128 --epochs 2 --gpu_enabled \\\n",
    "                             --model_path model_state_dict_41.71.pcl \\\n",
    "                             --data_path tiny-imagenet-200 \\\n",
    "                             --model_module_path model.py \\\n",
    "                             --model_checkpoint_path model_checkpoints \\\n",
    "                             --effective_batch_size 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Модель за 2 эпохи пробила прошлый скор:) по скорости просадки нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qbbbO7V0QeGT"
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# effective_batch_size = 1024\n",
    "# loader_batch_size = 32\n",
    "# batches_per_update = effective_batch_size / loader_batch_size # Updating weights after 8 forward passes\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size=loader_batch_size)\n",
    "\n",
    "# optimizer.zero_grad()\n",
    "\n",
    "# for batch_i, (batch_X, batch_y) in enumerate(dataloader):\n",
    "#     l = loss(model(batch_X), batch_y)\n",
    "#     l.backward() # Adds gradients\n",
    "  \n",
    "#     if (batch_i + 1) % batches_per_update == 0:\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZqxvZWH9Uxtq"
   },
   "source": [
    "**Task**. Explore the trade-off between computation time and memory usage while maintaining the same effective batch size. By effective batch size we mean the number of objects over which the loss is computed before taking a gradient step.\n",
    "\n",
    "**Requirements**\n",
    "\n",
    "* Compare compute between accumulating gradient and gradient checkpointing with similar memory consumptions\n",
    "* Incorporate gradient accumulation into your script with optional argument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K3iiJZuhSUR0"
   },
   "source": [
    "## Accuracy vs compute trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0WOWhqMJSboR"
   },
   "source": [
    "### Tensor type size\n",
    "\n",
    "One of the hyperparameter affecting memory consumption is the precision (e.g. floating point number). The most popular choice is 32 bit however with several hacks* 16 bit arithmetics can save you approximately half of the memory without considerable loss of perfomance. This is called mixed precision training.\n",
    "\n",
    "*https://arxiv.org/pdf/1710.03740.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xAEF9aJc-43"
   },
   "source": [
    "### Quantization\n",
    "\n",
    "We can actually move further and use even lower precision like 8-bit integers:\n",
    "\n",
    "* https://heartbeat.fritz.ai/8-bit-quantization-and-tensorflow-lite-speeding-up-mobile-inference-with-low-precision-a882dfcafbbd\n",
    "* https://nervanasystems.github.io/distiller/quantization/\n",
    "* https://arxiv.org/abs/1712.05877"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fXad1svpSk8f"
   },
   "source": [
    "### Knowledge distillation\n",
    "Suppose that we have a large network (*teacher network*) or an ensemble of networks which has a good accuracy. We can like train a much smaller network (*student network*) using the outputs of teacher networks. It turns out that the perfomance could be even better! This approach doesn't help with training speed, but can be quite beneficial when we'd like to reduce the model size for low-memory devices.\n",
    "\n",
    "* https://www.ttic.edu/dl/dark14.pdf\n",
    "* [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)\n",
    "* https://medium.com/neural-machines/knowledge-distillation-dc241d7c2322\n",
    "\n",
    "Even the completely different ([article](https://arxiv.org/abs/1711.10433)) architecture can be used in a student model, e.g. you can approximate an autoregressive model (WaveNet) by a non-autoregressive one.\n",
    "\n",
    "**Task**. Distill your (teacher) network with smaller one (student), compare it perfomance with the teacher network and with the same (student) trained directly from data.\n",
    "\n",
    "**Note**. Logits carry more information than the probabilities after softmax\n",
    "\n",
    "This approach doesn't help with training speed, but can be quite beneficial when we'd like to reduce the model size for low-memory devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imp import load_source\n",
    "import numpy as np\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.checkpoint import checkpoint_sequential\n",
    "    \n",
    "means = np.array([0.485, 0.456, 0.406])\n",
    "stds = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "def count_score(model, batch_gen, accuracy_list, gpu=False):\n",
    "    model.train(False) # disable dropout / use averages for batch_norm\n",
    "    for X_batch, y_batch in batch_gen:\n",
    "        if gpu:\n",
    "            logits = model(Variable(torch.FloatTensor(X_batch)).cuda())\n",
    "        else:\n",
    "            logits = model(Variable(torch.FloatTensor(X_batch)).cpu())\n",
    "\n",
    "        y_pred = logits.max(1)[1].data\n",
    "        accuracy_list.append(np.mean( (y_batch.cpu() == y_pred.cpu()).numpy() ))\n",
    "    return accuracy_list\n",
    "\n",
    "def train(student_model, teacher_model, opt, loss_fn, model_checkpoint_path, data_path, use_checkpoint=False, gpu=False, batch_size=128, epochs=100, effective_batch_size=None):\n",
    "    transform = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomRotation((-30,30)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(means, stds)\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(means, stds)\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    dataset = torchvision.datasets.ImageFolder(os.path.join(data_path, 'train'), transform=transform['train'])\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [80000, 20000])\n",
    "\n",
    "\n",
    "    train_batch_gen = torch.utils.data.DataLoader(train_dataset, \n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=True,\n",
    "                                                  num_workers=4)\n",
    "    val_batch_gen = torch.utils.data.DataLoader(val_dataset, \n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=True,\n",
    "                                                num_workers=2)\n",
    "    \n",
    "    train_loss = []\n",
    "    val_accuracy = []\n",
    "    \n",
    "    prev_val_acc =  0\n",
    "    \n",
    "    if effective_batch_size is None:\n",
    "        batches_per_update = 1\n",
    "    else:\n",
    "        batches_per_update = effective_batch_size / batch_size\n",
    "    \n",
    "    if use_checkpoint:\n",
    "        segments = 4\n",
    "        modules = [module for k, module in student_model._modules.items()]\n",
    "    \n",
    "    additional_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        start_time = time.time()\n",
    "        student_model.train(True) # enable dropout / batch_norm training behavior\n",
    "        forward_time = 0\n",
    "        backward_time = 0\n",
    "\n",
    "        for batch_i, (X_batch, y_batch) in enumerate(train_batch_gen):\n",
    "            # train on batch\n",
    "            start_counter = time.perf_counter()\n",
    "            if gpu:\n",
    "                X_batch = Variable(torch.FloatTensor(X_batch)).cuda()\n",
    "                y_batch = Variable(torch.LongTensor(y_batch)).cuda()\n",
    "                if use_checkpoint:\n",
    "                    raise NotImplementedError()\n",
    "                else:\n",
    "                    logits = student_model.cuda()(X_batch)\n",
    "                    teacher_logits = teacher_model.cuda()(X_batch)\n",
    "            else:\n",
    "                X_batch = Variable(torch.FloatTensor(X_batch)).cpu()\n",
    "                y_batch = Variable(torch.LongTensor(y_batch)).cpu()\n",
    "                if use_checkpoint:\n",
    "                    raise NotImplementedError()\n",
    "                else:\n",
    "                    logits = student_model.cpu()(X_batch)\n",
    "                    teacher_logits = teacher_model.cpu()(X_batch)\n",
    "\n",
    "            loss = 0.8*loss_fn(logits, teacher_logits) + 0.2* additional_criterion(logits, y_batch)\n",
    "\n",
    "            apply_counter = time.perf_counter()\n",
    "            forward_time += apply_counter - start_counter\n",
    "            \n",
    "            loss.backward()\n",
    "            backward_time += time.perf_counter() - apply_counter\n",
    "            \n",
    "            if (batch_i + 1) % batches_per_update == 0:\n",
    "                opt.step()\n",
    "                opt.zero_grad()\n",
    "\n",
    "            train_loss.append(loss.data.cpu().numpy())\n",
    "        \n",
    "        val_accuracy = count_score(student_model, batch_gen=val_batch_gen, accuracy_list=val_accuracy, gpu=gpu)\n",
    "        vall_acc =  np.mean(val_accuracy[-len(val_dataset) // batch_size :]) * 100\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, epochs, time.time() - start_time))\n",
    "        print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "            np.mean(train_loss[-len(train_dataset) // batch_size :])))\n",
    "        print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(vall_acc))\n",
    "        torch.save(student_model.state_dict(), os.path.join(model_checkpoint_path, \"model_{}_{:.2f}.pcl\".format(epoch, vall_acc)))\n",
    "        \n",
    "        print(\"\\t\\tForward pass took  {:.3f} seconds\".format(forward_time))\n",
    "        print(\"\\t\\tBackward pass took {:.3f} seconds\".format(backward_time))\n",
    "\n",
    "        if vall_acc > prev_val_acc:\n",
    "            prev_val_acc = vall_acc\n",
    "            print(\"Saving new best model!\")\n",
    "            torch.save(student_model.state_dict(), os.path.join(model_checkpoint_path, \"model_best.pcl\"))\n",
    "\n",
    "    return student_model\n",
    "\n",
    "def validate(model, data_path, batch_size):\n",
    "    transform = {\n",
    "        'test': transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(means, stds)\n",
    "        ])\n",
    "    }\n",
    "    \n",
    "    test_dataset = torchvision.datasets.ImageFolder(os.path.join(data_path, 'new_val'), transform=transform['test'])\n",
    "    test_batch_gen = torch.utils.data.DataLoader(test_dataset, \n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=False,\n",
    "                                                 num_workers=2)\n",
    "    \n",
    "    model.train(False) # disable dropout / use averages for batch_norm\n",
    "    test_acc = []\n",
    "\n",
    "    for X_batch, y_batch in test_batch_gen:\n",
    "        logits = model(Variable(torch.FloatTensor(X_batch)).cuda())\n",
    "        y_pred = logits.max(1)[1].data\n",
    "        test_acc += list((y_batch.cpu() == y_pred.cpu()).numpy())\n",
    "    \n",
    "    test_accuracy = np.mean(test_acc)\n",
    "    \n",
    "    print(\"Final results:\")\n",
    "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "        test_accuracy * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_enabled=True \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpu_enabled:\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "\n",
    "load_source(\"teacher_model\", \"model.py\") \n",
    "from teacher_model import get_model as get_teacher_model\n",
    "\n",
    "\n",
    "teacher_model, _, _ = get_teacher_model(model_path=\"model_checkpoints/model_best.pcl\", gpu=gpu_enabled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace)\n",
       "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU(inplace)\n",
       "  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): ReLU(inplace)\n",
       "  (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (13): ReLU(inplace)\n",
       "  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (15): Flatten()\n",
       "  (16): Linear(in_features=16384, out_features=1024, bias=True)\n",
       "  (17): ReLU(inplace)\n",
       "  (18): Dropout(p=0.5)\n",
       "  (19): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (20): ReLU(inplace)\n",
       "  (21): Dropout(p=0.5)\n",
       "  (22): Linear(in_features=512, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (batchnorm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (ReLU1): ReLU()\n",
       "  (MaxPool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (flatten): Flatten()\n",
       "  (dense1): Linear(in_features=32768, out_features=1000, bias=True)\n",
       "  (dense1_relu): ReLU()\n",
       "  (dropout1): Dropout(p=0.3)\n",
       "  (dense2): Linear(in_features=1000, out_features=512, bias=True)\n",
       "  (dense2_relu): ReLU()\n",
       "  (dropout2): Dropout(p=0.05)\n",
       "  (dense2_logits): Linear(in_features=512, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "    \n",
    "student_model = nn.Sequential()\n",
    "\n",
    "\n",
    "student_model.add_module('conv1', nn.Conv2d(3, 16, kernel_size=5, padding=2))\n",
    "student_model.add_module('batchnorm1', nn.BatchNorm2d(16))\n",
    "student_model.add_module('ReLU1', nn.ReLU())\n",
    "student_model.add_module('MaxPool1', nn.MaxPool2d(2))\n",
    "\n",
    "student_model.add_module('conv2', nn.Conv2d(16, 32, kernel_size=5, padding=2))\n",
    "student_model.add_module('batchnorm1', nn.BatchNorm2d(16))\n",
    "student_model.add_module('ReLU1', nn.ReLU())\n",
    "student_model.add_module('MaxPool1', nn.MaxPool2d(2))\n",
    "\n",
    "student_model.add_module('flatten', Flatten())\n",
    "\n",
    "student_model.add_module('dense1', nn.Linear(32768, 1000))\n",
    "student_model.add_module('dense1_relu', nn.ReLU())\n",
    "student_model.add_module('dropout1', nn.Dropout(0.3))\n",
    "\n",
    "student_model.add_module('dense2', nn.Linear(1000, 512))\n",
    "student_model.add_module('dense2_relu', nn.ReLU())\n",
    "student_model.add_module('dropout2', nn.Dropout(0.05))\n",
    "\n",
    "student_model.add_module('dense2_logits', nn.Linear(512, 200)) # logits for 200 classes\n",
    "student_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss,self).__init__()\n",
    "\n",
    "    def forward(self,x,y):\n",
    "        criterion = nn.MSELoss()\n",
    "        loss = torch.sqrt(criterion(x, y))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(student_model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 52.281s\n",
      "  training loss (in-iteration): \t12.182045\n",
      "  validation accuracy: \t\t\t0.84 %\n",
      "\t\tForward pass took  2.211 seconds\n",
      "\t\tBackward pass took 2.173 seconds\n",
      "Saving new best model!\n",
      "Epoch 2 of 100 took 53.234s\n",
      "  training loss (in-iteration): \t8.665157\n",
      "  validation accuracy: \t\t\t1.35 %\n",
      "\t\tForward pass took  2.207 seconds\n",
      "\t\tBackward pass took 2.500 seconds\n",
      "Saving new best model!\n",
      "Epoch 3 of 100 took 53.435s\n",
      "  training loss (in-iteration): \t8.121580\n",
      "  validation accuracy: \t\t\t2.13 %\n",
      "\t\tForward pass took  2.199 seconds\n",
      "\t\tBackward pass took 2.528 seconds\n",
      "Saving new best model!\n",
      "Epoch 4 of 100 took 53.435s\n",
      "  training loss (in-iteration): \t7.627388\n",
      "  validation accuracy: \t\t\t2.57 %\n",
      "\t\tForward pass took  2.209 seconds\n",
      "\t\tBackward pass took 2.588 seconds\n",
      "Saving new best model!\n",
      "Epoch 5 of 100 took 53.590s\n",
      "  training loss (in-iteration): \t7.233976\n",
      "  validation accuracy: \t\t\t3.04 %\n",
      "\t\tForward pass took  2.232 seconds\n",
      "\t\tBackward pass took 2.519 seconds\n",
      "Saving new best model!\n",
      "Epoch 6 of 100 took 53.571s\n",
      "  training loss (in-iteration): \t6.889088\n",
      "  validation accuracy: \t\t\t4.23 %\n",
      "\t\tForward pass took  2.208 seconds\n",
      "\t\tBackward pass took 2.407 seconds\n",
      "Saving new best model!\n",
      "Epoch 7 of 100 took 53.542s\n",
      "  training loss (in-iteration): \t6.547915\n",
      "  validation accuracy: \t\t\t4.78 %\n",
      "\t\tForward pass took  2.211 seconds\n",
      "\t\tBackward pass took 2.483 seconds\n",
      "Saving new best model!\n",
      "Epoch 8 of 100 took 53.544s\n",
      "  training loss (in-iteration): \t6.255708\n",
      "  validation accuracy: \t\t\t6.09 %\n",
      "\t\tForward pass took  2.230 seconds\n",
      "\t\tBackward pass took 2.459 seconds\n",
      "Saving new best model!\n",
      "Epoch 9 of 100 took 53.721s\n",
      "  training loss (in-iteration): \t6.062194\n",
      "  validation accuracy: \t\t\t6.53 %\n",
      "\t\tForward pass took  2.206 seconds\n",
      "\t\tBackward pass took 2.285 seconds\n",
      "Saving new best model!\n",
      "Epoch 10 of 100 took 53.698s\n",
      "  training loss (in-iteration): \t5.881347\n",
      "  validation accuracy: \t\t\t7.36 %\n",
      "\t\tForward pass took  2.207 seconds\n",
      "\t\tBackward pass took 2.211 seconds\n",
      "Saving new best model!\n",
      "Epoch 11 of 100 took 53.753s\n",
      "  training loss (in-iteration): \t5.633807\n",
      "  validation accuracy: \t\t\t8.19 %\n",
      "\t\tForward pass took  2.236 seconds\n",
      "\t\tBackward pass took 2.527 seconds\n",
      "Saving new best model!\n",
      "Epoch 12 of 100 took 53.632s\n",
      "  training loss (in-iteration): \t5.509961\n",
      "  validation accuracy: \t\t\t8.85 %\n",
      "\t\tForward pass took  2.235 seconds\n",
      "\t\tBackward pass took 2.505 seconds\n",
      "Saving new best model!\n",
      "Epoch 13 of 100 took 53.666s\n",
      "  training loss (in-iteration): \t5.400975\n",
      "  validation accuracy: \t\t\t9.18 %\n",
      "\t\tForward pass took  2.215 seconds\n",
      "\t\tBackward pass took 2.551 seconds\n",
      "Saving new best model!\n",
      "Epoch 14 of 100 took 53.694s\n",
      "  training loss (in-iteration): \t5.198022\n",
      "  validation accuracy: \t\t\t9.53 %\n",
      "\t\tForward pass took  2.216 seconds\n",
      "\t\tBackward pass took 2.563 seconds\n",
      "Saving new best model!\n",
      "Epoch 15 of 100 took 53.715s\n",
      "  training loss (in-iteration): \t5.069329\n",
      "  validation accuracy: \t\t\t10.12 %\n",
      "\t\tForward pass took  2.231 seconds\n",
      "\t\tBackward pass took 2.522 seconds\n",
      "Saving new best model!\n",
      "Epoch 16 of 100 took 53.847s\n",
      "  training loss (in-iteration): \t4.961126\n",
      "  validation accuracy: \t\t\t11.11 %\n",
      "\t\tForward pass took  2.237 seconds\n",
      "\t\tBackward pass took 2.528 seconds\n",
      "Saving new best model!\n",
      "Epoch 17 of 100 took 53.736s\n",
      "  training loss (in-iteration): \t4.835677\n",
      "  validation accuracy: \t\t\t11.93 %\n",
      "\t\tForward pass took  2.238 seconds\n",
      "\t\tBackward pass took 2.558 seconds\n",
      "Saving new best model!\n",
      "Epoch 18 of 100 took 53.998s\n",
      "  training loss (in-iteration): \t4.781215\n",
      "  validation accuracy: \t\t\t12.33 %\n",
      "\t\tForward pass took  2.214 seconds\n",
      "\t\tBackward pass took 2.422 seconds\n",
      "Saving new best model!\n",
      "Epoch 19 of 100 took 53.978s\n",
      "  training loss (in-iteration): \t4.641069\n",
      "  validation accuracy: \t\t\t12.27 %\n",
      "\t\tForward pass took  2.234 seconds\n",
      "\t\tBackward pass took 2.528 seconds\n",
      "Epoch 20 of 100 took 54.059s\n",
      "  training loss (in-iteration): \t4.622741\n",
      "  validation accuracy: \t\t\t13.34 %\n",
      "\t\tForward pass took  2.230 seconds\n",
      "\t\tBackward pass took 2.441 seconds\n",
      "Saving new best model!\n",
      "Epoch 21 of 100 took 53.980s\n",
      "  training loss (in-iteration): \t4.464022\n",
      "  validation accuracy: \t\t\t13.71 %\n",
      "\t\tForward pass took  2.212 seconds\n",
      "\t\tBackward pass took 2.626 seconds\n",
      "Saving new best model!\n",
      "Epoch 22 of 100 took 53.975s\n",
      "  training loss (in-iteration): \t4.421142\n",
      "  validation accuracy: \t\t\t14.21 %\n",
      "\t\tForward pass took  2.248 seconds\n",
      "\t\tBackward pass took 2.545 seconds\n",
      "Saving new best model!\n",
      "Epoch 23 of 100 took 53.779s\n",
      "  training loss (in-iteration): \t4.339520\n",
      "  validation accuracy: \t\t\t14.55 %\n",
      "\t\tForward pass took  2.214 seconds\n",
      "\t\tBackward pass took 2.633 seconds\n",
      "Saving new best model!\n",
      "Epoch 24 of 100 took 53.715s\n",
      "  training loss (in-iteration): \t4.291885\n",
      "  validation accuracy: \t\t\t15.24 %\n",
      "\t\tForward pass took  2.219 seconds\n",
      "\t\tBackward pass took 2.175 seconds\n",
      "Saving new best model!\n",
      "Epoch 25 of 100 took 53.808s\n",
      "  training loss (in-iteration): \t4.246558\n",
      "  validation accuracy: \t\t\t15.37 %\n",
      "\t\tForward pass took  2.209 seconds\n",
      "\t\tBackward pass took 2.555 seconds\n",
      "Saving new best model!\n",
      "Epoch 26 of 100 took 53.737s\n",
      "  training loss (in-iteration): \t4.194025\n",
      "  validation accuracy: \t\t\t15.53 %\n",
      "\t\tForward pass took  2.230 seconds\n",
      "\t\tBackward pass took 2.552 seconds\n",
      "Saving new best model!\n",
      "Epoch 27 of 100 took 53.786s\n",
      "  training loss (in-iteration): \t4.136277\n",
      "  validation accuracy: \t\t\t15.68 %\n",
      "\t\tForward pass took  2.214 seconds\n",
      "\t\tBackward pass took 2.282 seconds\n",
      "Saving new best model!\n",
      "Epoch 28 of 100 took 53.797s\n",
      "  training loss (in-iteration): \t4.106624\n",
      "  validation accuracy: \t\t\t15.88 %\n",
      "\t\tForward pass took  2.249 seconds\n",
      "\t\tBackward pass took 2.087 seconds\n",
      "Saving new best model!\n",
      "Epoch 29 of 100 took 53.722s\n",
      "  training loss (in-iteration): \t4.048030\n",
      "  validation accuracy: \t\t\t16.31 %\n",
      "\t\tForward pass took  2.224 seconds\n",
      "\t\tBackward pass took 2.213 seconds\n",
      "Saving new best model!\n",
      "Epoch 30 of 100 took 53.770s\n",
      "  training loss (in-iteration): \t4.022671\n",
      "  validation accuracy: \t\t\t16.75 %\n",
      "\t\tForward pass took  2.218 seconds\n",
      "\t\tBackward pass took 2.446 seconds\n",
      "Saving new best model!\n",
      "Epoch 31 of 100 took 53.664s\n",
      "  training loss (in-iteration): \t3.959079\n",
      "  validation accuracy: \t\t\t17.51 %\n",
      "\t\tForward pass took  2.218 seconds\n",
      "\t\tBackward pass took 2.379 seconds\n",
      "Saving new best model!\n",
      "Epoch 32 of 100 took 53.759s\n",
      "  training loss (in-iteration): \t3.907238\n",
      "  validation accuracy: \t\t\t17.69 %\n",
      "\t\tForward pass took  2.244 seconds\n",
      "\t\tBackward pass took 2.446 seconds\n",
      "Saving new best model!\n",
      "Epoch 33 of 100 took 53.875s\n",
      "  training loss (in-iteration): \t3.871685\n",
      "  validation accuracy: \t\t\t17.81 %\n",
      "\t\tForward pass took  2.240 seconds\n",
      "\t\tBackward pass took 2.499 seconds\n",
      "Saving new best model!\n",
      "Epoch 34 of 100 took 53.764s\n",
      "  training loss (in-iteration): \t3.824767\n",
      "  validation accuracy: \t\t\t17.75 %\n",
      "\t\tForward pass took  2.222 seconds\n",
      "\t\tBackward pass took 2.674 seconds\n",
      "Epoch 35 of 100 took 53.849s\n",
      "  training loss (in-iteration): \t3.822463\n",
      "  validation accuracy: \t\t\t17.88 %\n",
      "\t\tForward pass took  2.242 seconds\n",
      "\t\tBackward pass took 2.475 seconds\n",
      "Saving new best model!\n",
      "Epoch 36 of 100 took 53.829s\n",
      "  training loss (in-iteration): \t3.780081\n",
      "  validation accuracy: \t\t\t18.12 %\n",
      "\t\tForward pass took  2.249 seconds\n",
      "\t\tBackward pass took 2.434 seconds\n",
      "Saving new best model!\n",
      "Epoch 37 of 100 took 53.856s\n",
      "  training loss (in-iteration): \t3.754321\n",
      "  validation accuracy: \t\t\t18.70 %\n",
      "\t\tForward pass took  2.228 seconds\n",
      "\t\tBackward pass took 2.537 seconds\n",
      "Saving new best model!\n",
      "Epoch 38 of 100 took 53.732s\n",
      "  training loss (in-iteration): \t3.684152\n",
      "  validation accuracy: \t\t\t18.58 %\n",
      "\t\tForward pass took  2.217 seconds\n",
      "\t\tBackward pass took 2.528 seconds\n",
      "Epoch 39 of 100 took 53.814s\n",
      "  training loss (in-iteration): \t3.688724\n",
      "  validation accuracy: \t\t\t18.47 %\n",
      "\t\tForward pass took  2.232 seconds\n",
      "\t\tBackward pass took 2.555 seconds\n",
      "Epoch 40 of 100 took 53.783s\n",
      "  training loss (in-iteration): \t3.679653\n",
      "  validation accuracy: \t\t\t19.59 %\n",
      "\t\tForward pass took  2.217 seconds\n",
      "\t\tBackward pass took 2.352 seconds\n",
      "Saving new best model!\n",
      "Epoch 41 of 100 took 53.860s\n",
      "  training loss (in-iteration): \t3.653469\n",
      "  validation accuracy: \t\t\t19.04 %\n",
      "\t\tForward pass took  2.248 seconds\n",
      "\t\tBackward pass took 2.512 seconds\n",
      "Epoch 42 of 100 took 53.868s\n",
      "  training loss (in-iteration): \t3.620399\n",
      "  validation accuracy: \t\t\t19.09 %\n",
      "\t\tForward pass took  2.233 seconds\n",
      "\t\tBackward pass took 2.566 seconds\n",
      "Epoch 43 of 100 took 53.850s\n",
      "  training loss (in-iteration): \t3.578733\n",
      "  validation accuracy: \t\t\t19.65 %\n",
      "\t\tForward pass took  2.224 seconds\n",
      "\t\tBackward pass took 2.331 seconds\n",
      "Saving new best model!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 of 100 took 53.811s\n",
      "  training loss (in-iteration): \t3.543021\n",
      "  validation accuracy: \t\t\t19.86 %\n",
      "\t\tForward pass took  2.226 seconds\n",
      "\t\tBackward pass took 1.916 seconds\n",
      "Saving new best model!\n",
      "Epoch 45 of 100 took 53.900s\n",
      "  training loss (in-iteration): \t3.505640\n",
      "  validation accuracy: \t\t\t20.28 %\n",
      "\t\tForward pass took  2.233 seconds\n",
      "\t\tBackward pass took 2.307 seconds\n",
      "Saving new best model!\n",
      "Epoch 46 of 100 took 53.854s\n",
      "  training loss (in-iteration): \t3.494891\n",
      "  validation accuracy: \t\t\t20.31 %\n",
      "\t\tForward pass took  2.219 seconds\n",
      "\t\tBackward pass took 2.657 seconds\n",
      "Saving new best model!\n",
      "Epoch 47 of 100 took 53.736s\n",
      "  training loss (in-iteration): \t3.490957\n",
      "  validation accuracy: \t\t\t20.65 %\n",
      "\t\tForward pass took  2.221 seconds\n",
      "\t\tBackward pass took 2.579 seconds\n",
      "Saving new best model!\n",
      "Epoch 48 of 100 took 53.862s\n",
      "  training loss (in-iteration): \t3.444209\n",
      "  validation accuracy: \t\t\t20.72 %\n",
      "\t\tForward pass took  2.224 seconds\n",
      "\t\tBackward pass took 2.599 seconds\n",
      "Saving new best model!\n",
      "Epoch 49 of 100 took 53.813s\n",
      "  training loss (in-iteration): \t3.447315\n",
      "  validation accuracy: \t\t\t20.58 %\n",
      "\t\tForward pass took  2.247 seconds\n",
      "\t\tBackward pass took 2.464 seconds\n",
      "Epoch 50 of 100 took 53.954s\n",
      "  training loss (in-iteration): \t3.435603\n",
      "  validation accuracy: \t\t\t20.54 %\n",
      "\t\tForward pass took  2.249 seconds\n",
      "\t\tBackward pass took 2.439 seconds\n",
      "Epoch 51 of 100 took 53.813s\n",
      "  training loss (in-iteration): \t3.390463\n",
      "  validation accuracy: \t\t\t21.18 %\n",
      "\t\tForward pass took  2.240 seconds\n",
      "\t\tBackward pass took 2.286 seconds\n",
      "Saving new best model!\n",
      "Epoch 52 of 100 took 53.735s\n",
      "  training loss (in-iteration): \t3.374473\n",
      "  validation accuracy: \t\t\t20.90 %\n",
      "\t\tForward pass took  2.231 seconds\n",
      "\t\tBackward pass took 2.299 seconds\n",
      "Epoch 53 of 100 took 53.874s\n",
      "  training loss (in-iteration): \t3.343703\n",
      "  validation accuracy: \t\t\t21.01 %\n",
      "\t\tForward pass took  2.244 seconds\n",
      "\t\tBackward pass took 2.599 seconds\n",
      "Epoch 54 of 100 took 53.884s\n",
      "  training loss (in-iteration): \t3.336323\n",
      "  validation accuracy: \t\t\t20.86 %\n",
      "\t\tForward pass took  2.237 seconds\n",
      "\t\tBackward pass took 2.393 seconds\n",
      "Epoch 55 of 100 took 53.749s\n",
      "  training loss (in-iteration): \t3.332669\n",
      "  validation accuracy: \t\t\t21.26 %\n",
      "\t\tForward pass took  2.253 seconds\n",
      "\t\tBackward pass took 2.391 seconds\n",
      "Saving new best model!\n",
      "Epoch 56 of 100 took 53.737s\n",
      "  training loss (in-iteration): \t3.322641\n",
      "  validation accuracy: \t\t\t21.52 %\n",
      "\t\tForward pass took  2.238 seconds\n",
      "\t\tBackward pass took 2.426 seconds\n",
      "Saving new best model!\n",
      "Epoch 57 of 100 took 53.865s\n",
      "  training loss (in-iteration): \t3.264840\n",
      "  validation accuracy: \t\t\t21.65 %\n",
      "\t\tForward pass took  2.227 seconds\n",
      "\t\tBackward pass took 2.274 seconds\n",
      "Saving new best model!\n",
      "Epoch 58 of 100 took 53.767s\n",
      "  training loss (in-iteration): \t3.240123\n",
      "  validation accuracy: \t\t\t21.46 %\n",
      "\t\tForward pass took  2.232 seconds\n",
      "\t\tBackward pass took 2.471 seconds\n",
      "Epoch 59 of 100 took 53.869s\n",
      "  training loss (in-iteration): \t3.237402\n",
      "  validation accuracy: \t\t\t21.36 %\n",
      "\t\tForward pass took  2.241 seconds\n",
      "\t\tBackward pass took 2.595 seconds\n",
      "Epoch 60 of 100 took 53.794s\n",
      "  training loss (in-iteration): \t3.213039\n",
      "  validation accuracy: \t\t\t21.78 %\n",
      "\t\tForward pass took  2.231 seconds\n",
      "\t\tBackward pass took 2.575 seconds\n",
      "Saving new best model!\n",
      "Epoch 61 of 100 took 53.829s\n",
      "  training loss (in-iteration): \t3.198087\n",
      "  validation accuracy: \t\t\t21.66 %\n",
      "\t\tForward pass took  2.251 seconds\n",
      "\t\tBackward pass took 2.612 seconds\n",
      "Epoch 62 of 100 took 53.847s\n",
      "  training loss (in-iteration): \t3.197479\n",
      "  validation accuracy: \t\t\t22.04 %\n",
      "\t\tForward pass took  2.237 seconds\n",
      "\t\tBackward pass took 2.426 seconds\n",
      "Saving new best model!\n",
      "Epoch 63 of 100 took 53.756s\n",
      "  training loss (in-iteration): \t3.155262\n",
      "  validation accuracy: \t\t\t22.14 %\n",
      "\t\tForward pass took  2.218 seconds\n",
      "\t\tBackward pass took 1.386 seconds\n",
      "Saving new best model!\n",
      "Epoch 64 of 100 took 53.805s\n",
      "  training loss (in-iteration): \t3.195267\n",
      "  validation accuracy: \t\t\t22.02 %\n",
      "\t\tForward pass took  2.234 seconds\n",
      "\t\tBackward pass took 2.475 seconds\n",
      "Epoch 65 of 100 took 53.885s\n",
      "  training loss (in-iteration): \t3.151645\n",
      "  validation accuracy: \t\t\t22.28 %\n",
      "\t\tForward pass took  2.234 seconds\n",
      "\t\tBackward pass took 2.614 seconds\n",
      "Saving new best model!\n",
      "Epoch 66 of 100 took 53.915s\n",
      "  training loss (in-iteration): \t3.132724\n",
      "  validation accuracy: \t\t\t22.76 %\n",
      "\t\tForward pass took  2.250 seconds\n",
      "\t\tBackward pass took 2.461 seconds\n",
      "Saving new best model!\n",
      "Epoch 67 of 100 took 53.943s\n",
      "  training loss (in-iteration): \t3.135242\n",
      "  validation accuracy: \t\t\t22.42 %\n",
      "\t\tForward pass took  2.243 seconds\n",
      "\t\tBackward pass took 2.617 seconds\n",
      "Epoch 68 of 100 took 53.854s\n",
      "  training loss (in-iteration): \t3.132876\n",
      "  validation accuracy: \t\t\t22.75 %\n",
      "\t\tForward pass took  2.242 seconds\n",
      "\t\tBackward pass took 2.546 seconds\n",
      "Epoch 69 of 100 took 53.916s\n",
      "  training loss (in-iteration): \t3.074429\n",
      "  validation accuracy: \t\t\t22.77 %\n",
      "\t\tForward pass took  2.231 seconds\n",
      "\t\tBackward pass took 2.535 seconds\n",
      "Saving new best model!\n",
      "Epoch 70 of 100 took 53.764s\n",
      "  training loss (in-iteration): \t3.086212\n",
      "  validation accuracy: \t\t\t22.42 %\n",
      "\t\tForward pass took  2.225 seconds\n",
      "\t\tBackward pass took 2.402 seconds\n",
      "Epoch 71 of 100 took 53.918s\n",
      "  training loss (in-iteration): \t3.080445\n",
      "  validation accuracy: \t\t\t22.63 %\n",
      "\t\tForward pass took  2.233 seconds\n",
      "\t\tBackward pass took 2.457 seconds\n",
      "Epoch 72 of 100 took 53.839s\n",
      "  training loss (in-iteration): \t3.060047\n",
      "  validation accuracy: \t\t\t22.97 %\n",
      "\t\tForward pass took  2.262 seconds\n",
      "\t\tBackward pass took 2.451 seconds\n",
      "Saving new best model!\n",
      "Epoch 73 of 100 took 53.761s\n",
      "  training loss (in-iteration): \t3.049279\n",
      "  validation accuracy: \t\t\t22.82 %\n",
      "\t\tForward pass took  2.232 seconds\n",
      "\t\tBackward pass took 2.389 seconds\n",
      "Epoch 74 of 100 took 53.861s\n",
      "  training loss (in-iteration): \t3.046768\n",
      "  validation accuracy: \t\t\t22.85 %\n",
      "\t\tForward pass took  2.245 seconds\n",
      "\t\tBackward pass took 2.537 seconds\n",
      "Epoch 77 of 100 took 53.827s\n",
      "  training loss (in-iteration): \t2.997320\n",
      "  validation accuracy: \t\t\t23.19 %\n",
      "\t\tForward pass took  2.249 seconds\n",
      "\t\tBackward pass took 2.121 seconds\n",
      "Saving new best model!\n",
      "Epoch 78 of 100 took 53.893s\n",
      "  training loss (in-iteration): \t2.998797\n",
      "  validation accuracy: \t\t\t23.37 %\n",
      "\t\tForward pass took  2.256 seconds\n",
      "\t\tBackward pass took 2.511 seconds\n",
      "Saving new best model!\n",
      "Epoch 79 of 100 took 53.852s\n",
      "  training loss (in-iteration): \t3.000275\n",
      "  validation accuracy: \t\t\t23.11 %\n",
      "\t\tForward pass took  2.244 seconds\n",
      "\t\tBackward pass took 2.503 seconds\n",
      "Epoch 80 of 100 took 53.863s\n",
      "  training loss (in-iteration): \t2.989687\n",
      "  validation accuracy: \t\t\t23.77 %\n",
      "\t\tForward pass took  2.232 seconds\n",
      "\t\tBackward pass took 2.465 seconds\n",
      "Saving new best model!\n",
      "Epoch 81 of 100 took 53.889s\n",
      "  training loss (in-iteration): \t2.971986\n",
      "  validation accuracy: \t\t\t23.62 %\n",
      "\t\tForward pass took  2.248 seconds\n",
      "\t\tBackward pass took 2.243 seconds\n",
      "Epoch 82 of 100 took 53.813s\n",
      "  training loss (in-iteration): \t2.990503\n",
      "  validation accuracy: \t\t\t23.56 %\n",
      "\t\tForward pass took  2.225 seconds\n",
      "\t\tBackward pass took 2.070 seconds\n",
      "Epoch 83 of 100 took 53.803s\n",
      "  training loss (in-iteration): \t2.942465\n",
      "  validation accuracy: \t\t\t23.22 %\n",
      "\t\tForward pass took  2.265 seconds\n",
      "\t\tBackward pass took 2.440 seconds\n",
      "Epoch 84 of 100 took 53.968s\n",
      "  training loss (in-iteration): \t2.935487\n",
      "  validation accuracy: \t\t\t23.99 %\n",
      "\t\tForward pass took  2.242 seconds\n",
      "\t\tBackward pass took 2.619 seconds\n",
      "Saving new best model!\n",
      "Epoch 85 of 100 took 53.705s\n",
      "  training loss (in-iteration): \t2.930428\n",
      "  validation accuracy: \t\t\t23.15 %\n",
      "\t\tForward pass took  2.255 seconds\n",
      "\t\tBackward pass took 2.403 seconds\n",
      "Epoch 86 of 100 took 53.788s\n",
      "  training loss (in-iteration): \t2.930491\n",
      "  validation accuracy: \t\t\t23.56 %\n",
      "\t\tForward pass took  2.240 seconds\n",
      "\t\tBackward pass took 2.208 seconds\n",
      "Epoch 87 of 100 took 53.899s\n",
      "  training loss (in-iteration): \t2.893059\n",
      "  validation accuracy: \t\t\t23.74 %\n",
      "\t\tForward pass took  2.265 seconds\n",
      "\t\tBackward pass took 2.361 seconds\n",
      "Epoch 88 of 100 took 53.882s\n",
      "  training loss (in-iteration): \t2.905033\n",
      "  validation accuracy: \t\t\t23.77 %\n",
      "\t\tForward pass took  2.250 seconds\n",
      "\t\tBackward pass took 2.187 seconds\n",
      "Epoch 89 of 100 took 53.890s\n",
      "  training loss (in-iteration): \t2.911650\n",
      "  validation accuracy: \t\t\t23.73 %\n",
      "\t\tForward pass took  2.261 seconds\n",
      "\t\tBackward pass took 2.392 seconds\n",
      "Epoch 90 of 100 took 53.871s\n",
      "  training loss (in-iteration): \t2.897994\n",
      "  validation accuracy: \t\t\t23.17 %\n",
      "\t\tForward pass took  2.255 seconds\n",
      "\t\tBackward pass took 2.587 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91 of 100 took 53.894s\n",
      "  training loss (in-iteration): \t2.867838\n",
      "  validation accuracy: \t\t\t23.78 %\n",
      "\t\tForward pass took  2.263 seconds\n",
      "\t\tBackward pass took 2.452 seconds\n",
      "Epoch 92 of 100 took 53.753s\n",
      "  training loss (in-iteration): \t2.848913\n",
      "  validation accuracy: \t\t\t24.03 %\n",
      "\t\tForward pass took  2.244 seconds\n",
      "\t\tBackward pass took 2.332 seconds\n",
      "Saving new best model!\n",
      "Epoch 93 of 100 took 53.786s\n",
      "  training loss (in-iteration): \t2.854687\n",
      "  validation accuracy: \t\t\t23.87 %\n",
      "\t\tForward pass took  2.240 seconds\n",
      "\t\tBackward pass took 2.353 seconds\n",
      "Epoch 94 of 100 took 53.809s\n",
      "  training loss (in-iteration): \t2.862361\n",
      "  validation accuracy: \t\t\t23.97 %\n",
      "\t\tForward pass took  2.241 seconds\n",
      "\t\tBackward pass took 2.337 seconds\n",
      "Epoch 95 of 100 took 53.795s\n",
      "  training loss (in-iteration): \t2.815340\n",
      "  validation accuracy: \t\t\t24.38 %\n",
      "\t\tForward pass took  2.237 seconds\n",
      "\t\tBackward pass took 2.554 seconds\n",
      "Saving new best model!\n",
      "Epoch 96 of 100 took 53.847s\n",
      "  training loss (in-iteration): \t2.823910\n",
      "  validation accuracy: \t\t\t24.68 %\n",
      "\t\tForward pass took  2.254 seconds\n",
      "\t\tBackward pass took 2.577 seconds\n",
      "Saving new best model!\n",
      "Epoch 97 of 100 took 53.873s\n",
      "  training loss (in-iteration): \t2.837713\n",
      "  validation accuracy: \t\t\t23.89 %\n",
      "\t\tForward pass took  2.238 seconds\n",
      "\t\tBackward pass took 2.292 seconds\n",
      "Epoch 98 of 100 took 53.857s\n",
      "  training loss (in-iteration): \t2.818432\n",
      "  validation accuracy: \t\t\t24.77 %\n",
      "\t\tForward pass took  2.240 seconds\n",
      "\t\tBackward pass took 2.573 seconds\n",
      "Saving new best model!\n",
      "Epoch 99 of 100 took 53.880s\n",
      "  training loss (in-iteration): \t2.821440\n",
      "  validation accuracy: \t\t\t24.50 %\n",
      "\t\tForward pass took  2.232 seconds\n",
      "\t\tBackward pass took 2.515 seconds\n",
      "Epoch 100 of 100 took 53.951s\n",
      "  training loss (in-iteration): \t2.797626\n",
      "  validation accuracy: \t\t\t24.47 %\n",
      "\t\tForward pass took  2.241 seconds\n",
      "\t\tBackward pass took 2.419 seconds\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-cf59f6158239>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                       \u001b[0muse_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                       effective_batch_size=1024)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tiny-imagenet-200'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-1970f0617f5b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(student_model, teacher_model, opt, loss_fn, model_checkpoint_path, data_path, use_checkpoint, gpu, batch_size, epochs, effective_batch_size)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_best.pcl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "student_model = train(student_model=student_model, teacher_model=teacher_model, \n",
    "                      opt=opt, loss_fn=loss_fn, \n",
    "                      model_checkpoint_path='student_checkpoints', \n",
    "                      data_path='tiny-imagenet-200', \n",
    "                      gpu=gpu_enabled, \n",
    "                      batch_size=128, \n",
    "                      epochs=100, \n",
    "                      use_checkpoint=False, \n",
    "                      effective_batch_size=1024)\n",
    "validate(student_model, data_path='tiny-imagenet-200', batch_size=128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 51.330s\n",
      "  training loss (in-iteration): \t2.394271\n",
      "  validation accuracy: \t\t\t36.27 %\n",
      "\t\tForward pass took  2.292 seconds\n",
      "\t\tBackward pass took 2.649 seconds\n",
      "Saving new best model!\n",
      "Epoch 2 of 100 took 51.964s\n",
      "  training loss (in-iteration): \t2.376740\n",
      "  validation accuracy: \t\t\t36.09 %\n",
      "\t\tForward pass took  2.283 seconds\n",
      "\t\tBackward pass took 2.710 seconds\n",
      "Epoch 3 of 100 took 52.887s\n",
      "  training loss (in-iteration): \t2.363281\n",
      "  validation accuracy: \t\t\t35.52 %\n",
      "\t\tForward pass took  2.318 seconds\n",
      "\t\tBackward pass took 2.696 seconds\n",
      "Epoch 4 of 100 took 53.325s\n",
      "  training loss (in-iteration): \t2.360155\n",
      "  validation accuracy: \t\t\t35.16 %\n",
      "\t\tForward pass took  2.295 seconds\n",
      "\t\tBackward pass took 2.724 seconds\n",
      "Epoch 5 of 100 took 53.522s\n",
      "  training loss (in-iteration): \t2.357803\n",
      "  validation accuracy: \t\t\t35.10 %\n",
      "\t\tForward pass took  2.309 seconds\n",
      "\t\tBackward pass took 2.715 seconds\n",
      "Epoch 6 of 100 took 53.559s\n",
      "  training loss (in-iteration): \t2.371397\n",
      "  validation accuracy: \t\t\t34.64 %\n",
      "\t\tForward pass took  2.314 seconds\n",
      "\t\tBackward pass took 2.767 seconds\n",
      "Epoch 7 of 100 took 53.665s\n",
      "  training loss (in-iteration): \t2.351444\n",
      "  validation accuracy: \t\t\t34.87 %\n",
      "\t\tForward pass took  2.317 seconds\n",
      "\t\tBackward pass took 2.678 seconds\n",
      "Epoch 8 of 100 took 53.623s\n",
      "  training loss (in-iteration): \t2.340275\n",
      "  validation accuracy: \t\t\t34.51 %\n",
      "\t\tForward pass took  2.303 seconds\n",
      "\t\tBackward pass took 2.710 seconds\n",
      "Epoch 9 of 100 took 53.693s\n",
      "  training loss (in-iteration): \t2.350688\n",
      "  validation accuracy: \t\t\t34.26 %\n",
      "\t\tForward pass took  2.310 seconds\n",
      "\t\tBackward pass took 2.666 seconds\n",
      "Epoch 10 of 100 took 53.576s\n",
      "  training loss (in-iteration): \t2.353472\n",
      "  validation accuracy: \t\t\t34.84 %\n",
      "\t\tForward pass took  2.328 seconds\n",
      "\t\tBackward pass took 2.537 seconds\n",
      "Epoch 11 of 100 took 53.705s\n",
      "  training loss (in-iteration): \t2.340692\n",
      "  validation accuracy: \t\t\t34.36 %\n",
      "\t\tForward pass took  2.316 seconds\n",
      "\t\tBackward pass took 2.530 seconds\n",
      "Epoch 12 of 100 took 53.644s\n",
      "  training loss (in-iteration): \t2.330052\n",
      "  validation accuracy: \t\t\t34.35 %\n",
      "\t\tForward pass took  2.315 seconds\n",
      "\t\tBackward pass took 2.601 seconds\n",
      "Epoch 13 of 100 took 53.619s\n",
      "  training loss (in-iteration): \t2.321845\n",
      "  validation accuracy: \t\t\t33.85 %\n",
      "\t\tForward pass took  2.332 seconds\n",
      "\t\tBackward pass took 2.655 seconds\n",
      "Epoch 14 of 100 took 53.693s\n",
      "  training loss (in-iteration): \t2.323056\n",
      "  validation accuracy: \t\t\t34.08 %\n",
      "\t\tForward pass took  2.313 seconds\n",
      "\t\tBackward pass took 2.680 seconds\n",
      "Epoch 15 of 100 took 53.790s\n",
      "  training loss (in-iteration): \t2.322544\n",
      "  validation accuracy: \t\t\t33.84 %\n",
      "\t\tForward pass took  2.324 seconds\n",
      "\t\tBackward pass took 2.453 seconds\n",
      "Epoch 16 of 100 took 53.710s\n",
      "  training loss (in-iteration): \t2.325905\n",
      "  validation accuracy: \t\t\t33.95 %\n",
      "\t\tForward pass took  2.325 seconds\n",
      "\t\tBackward pass took 2.664 seconds\n",
      "Epoch 17 of 100 took 53.771s\n",
      "  training loss (in-iteration): \t2.321586\n",
      "  validation accuracy: \t\t\t34.28 %\n",
      "\t\tForward pass took  2.317 seconds\n",
      "\t\tBackward pass took 2.478 seconds\n",
      "Epoch 18 of 100 took 53.685s\n",
      "  training loss (in-iteration): \t2.324177\n",
      "  validation accuracy: \t\t\t34.07 %\n",
      "\t\tForward pass took  2.326 seconds\n",
      "\t\tBackward pass took 2.494 seconds\n",
      "Epoch 19 of 100 took 53.748s\n",
      "  training loss (in-iteration): \t2.304626\n",
      "  validation accuracy: \t\t\t33.76 %\n",
      "\t\tForward pass took  2.313 seconds\n",
      "\t\tBackward pass took 2.670 seconds\n",
      "Epoch 20 of 100 took 53.735s\n",
      "  training loss (in-iteration): \t2.308937\n",
      "  validation accuracy: \t\t\t33.62 %\n",
      "\t\tForward pass took  2.345 seconds\n",
      "\t\tBackward pass took 2.624 seconds\n",
      "Epoch 21 of 100 took 53.793s\n",
      "  training loss (in-iteration): \t2.306199\n",
      "  validation accuracy: \t\t\t32.92 %\n",
      "\t\tForward pass took  2.325 seconds\n",
      "\t\tBackward pass took 2.515 seconds\n",
      "Epoch 22 of 100 took 53.788s\n",
      "  training loss (in-iteration): \t2.304743\n",
      "  validation accuracy: \t\t\t33.12 %\n",
      "\t\tForward pass took  2.311 seconds\n",
      "\t\tBackward pass took 2.532 seconds\n",
      "Epoch 23 of 100 took 53.895s\n",
      "  training loss (in-iteration): \t2.312733\n",
      "  validation accuracy: \t\t\t33.16 %\n",
      "\t\tForward pass took  2.335 seconds\n",
      "\t\tBackward pass took 2.630 seconds\n",
      "Epoch 24 of 100 took 53.939s\n",
      "  training loss (in-iteration): \t2.306834\n",
      "  validation accuracy: \t\t\t33.28 %\n",
      "\t\tForward pass took  2.345 seconds\n",
      "\t\tBackward pass took 2.632 seconds\n",
      "Epoch 25 of 100 took 53.883s\n",
      "  training loss (in-iteration): \t2.302932\n",
      "  validation accuracy: \t\t\t33.12 %\n",
      "\t\tForward pass took  2.333 seconds\n",
      "\t\tBackward pass took 2.638 seconds\n",
      "Epoch 26 of 100 took 53.883s\n",
      "  training loss (in-iteration): \t2.306947\n",
      "  validation accuracy: \t\t\t33.34 %\n",
      "\t\tForward pass took  2.338 seconds\n",
      "\t\tBackward pass took 2.463 seconds\n",
      "Epoch 27 of 100 took 53.793s\n",
      "  training loss (in-iteration): \t2.294190\n",
      "  validation accuracy: \t\t\t33.02 %\n",
      "\t\tForward pass took  2.325 seconds\n",
      "\t\tBackward pass took 2.693 seconds\n",
      "Epoch 28 of 100 took 53.931s\n",
      "  training loss (in-iteration): \t2.294020\n",
      "  validation accuracy: \t\t\t33.26 %\n",
      "\t\tForward pass took  2.351 seconds\n",
      "\t\tBackward pass took 2.585 seconds\n",
      "Epoch 29 of 100 took 53.817s\n",
      "  training loss (in-iteration): \t2.302896\n",
      "  validation accuracy: \t\t\t33.06 %\n",
      "\t\tForward pass took  2.317 seconds\n",
      "\t\tBackward pass took 2.654 seconds\n",
      "Epoch 30 of 100 took 53.956s\n",
      "  training loss (in-iteration): \t2.304417\n",
      "  validation accuracy: \t\t\t33.17 %\n",
      "\t\tForward pass took  2.353 seconds\n",
      "\t\tBackward pass took 2.630 seconds\n",
      "Epoch 31 of 100 took 53.845s\n",
      "  training loss (in-iteration): \t2.304456\n",
      "  validation accuracy: \t\t\t33.10 %\n",
      "\t\tForward pass took  2.335 seconds\n",
      "\t\tBackward pass took 2.738 seconds\n",
      "Epoch 32 of 100 took 53.843s\n",
      "  training loss (in-iteration): \t2.284209\n",
      "  validation accuracy: \t\t\t32.84 %\n",
      "\t\tForward pass took  2.337 seconds\n",
      "\t\tBackward pass took 2.587 seconds\n",
      "Epoch 33 of 100 took 53.941s\n",
      "  training loss (in-iteration): \t2.293755\n",
      "  validation accuracy: \t\t\t32.58 %\n",
      "\t\tForward pass took  2.344 seconds\n",
      "\t\tBackward pass took 2.724 seconds\n",
      "Epoch 34 of 100 took 53.935s\n",
      "  training loss (in-iteration): \t2.297514\n",
      "  validation accuracy: \t\t\t32.62 %\n",
      "\t\tForward pass took  2.347 seconds\n",
      "\t\tBackward pass took 2.541 seconds\n",
      "Epoch 35 of 100 took 53.734s\n",
      "  training loss (in-iteration): \t2.285677\n",
      "  validation accuracy: \t\t\t32.67 %\n",
      "\t\tForward pass took  2.322 seconds\n",
      "\t\tBackward pass took 2.685 seconds\n",
      "Epoch 36 of 100 took 53.958s\n",
      "  training loss (in-iteration): \t2.282422\n",
      "  validation accuracy: \t\t\t32.78 %\n",
      "\t\tForward pass took  2.341 seconds\n",
      "\t\tBackward pass took 2.607 seconds\n",
      "Epoch 37 of 100 took 53.871s\n",
      "  training loss (in-iteration): \t2.293355\n",
      "  validation accuracy: \t\t\t32.68 %\n",
      "\t\tForward pass took  2.296 seconds\n",
      "\t\tBackward pass took 1.553 seconds\n",
      "Epoch 38 of 100 took 53.874s\n",
      "  training loss (in-iteration): \t2.298618\n",
      "  validation accuracy: \t\t\t32.35 %\n",
      "\t\tForward pass took  2.322 seconds\n",
      "\t\tBackward pass took 2.544 seconds\n",
      "Epoch 39 of 100 took 53.913s\n",
      "  training loss (in-iteration): \t2.284901\n",
      "  validation accuracy: \t\t\t32.21 %\n",
      "\t\tForward pass took  2.330 seconds\n",
      "\t\tBackward pass took 1.973 seconds\n",
      "Epoch 40 of 100 took 53.883s\n",
      "  training loss (in-iteration): \t2.288241\n",
      "  validation accuracy: \t\t\t32.71 %\n",
      "\t\tForward pass took  2.349 seconds\n",
      "\t\tBackward pass took 2.491 seconds\n",
      "Epoch 41 of 100 took 53.942s\n",
      "  training loss (in-iteration): \t2.275782\n",
      "  validation accuracy: \t\t\t32.54 %\n",
      "\t\tForward pass took  2.322 seconds\n",
      "\t\tBackward pass took 2.551 seconds\n",
      "Epoch 42 of 100 took 53.799s\n",
      "  training loss (in-iteration): \t2.281839\n",
      "  validation accuracy: \t\t\t32.32 %\n",
      "\t\tForward pass took  2.319 seconds\n",
      "\t\tBackward pass took 2.766 seconds\n",
      "Epoch 43 of 100 took 53.963s\n",
      "  training loss (in-iteration): \t2.272852\n",
      "  validation accuracy: \t\t\t32.97 %\n",
      "\t\tForward pass took  2.311 seconds\n",
      "\t\tBackward pass took 2.726 seconds\n",
      "Epoch 44 of 100 took 53.934s\n",
      "  training loss (in-iteration): \t2.284250\n",
      "  validation accuracy: \t\t\t32.40 %\n",
      "\t\tForward pass took  2.330 seconds\n",
      "\t\tBackward pass took 2.685 seconds\n",
      "Epoch 45 of 100 took 53.966s\n",
      "  training loss (in-iteration): \t2.277379\n",
      "  validation accuracy: \t\t\t32.21 %\n",
      "\t\tForward pass took  2.344 seconds\n",
      "\t\tBackward pass took 2.650 seconds\n",
      "Epoch 46 of 100 took 53.915s\n",
      "  training loss (in-iteration): \t2.276162\n",
      "  validation accuracy: \t\t\t32.13 %\n",
      "\t\tForward pass took  2.322 seconds\n",
      "\t\tBackward pass took 2.417 seconds\n",
      "Epoch 47 of 100 took 53.918s\n",
      "  training loss (in-iteration): \t2.274311\n",
      "  validation accuracy: \t\t\t31.94 %\n",
      "\t\tForward pass took  2.358 seconds\n",
      "\t\tBackward pass took 2.673 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 of 100 took 53.946s\n",
      "  training loss (in-iteration): \t2.271876\n",
      "  validation accuracy: \t\t\t32.26 %\n",
      "\t\tForward pass took  2.332 seconds\n",
      "\t\tBackward pass took 2.164 seconds\n",
      "Epoch 49 of 100 took 53.955s\n",
      "  training loss (in-iteration): \t2.274115\n",
      "  validation accuracy: \t\t\t32.23 %\n",
      "\t\tForward pass took  2.329 seconds\n",
      "\t\tBackward pass took 2.356 seconds\n",
      "Epoch 50 of 100 took 54.047s\n",
      "  training loss (in-iteration): \t2.275790\n",
      "  validation accuracy: \t\t\t31.96 %\n",
      "\t\tForward pass took  2.339 seconds\n",
      "\t\tBackward pass took 2.506 seconds\n",
      "Epoch 51 of 100 took 54.042s\n",
      "  training loss (in-iteration): \t2.270029\n",
      "  validation accuracy: \t\t\t32.13 %\n",
      "\t\tForward pass took  2.329 seconds\n",
      "\t\tBackward pass took 2.638 seconds\n",
      "Epoch 52 of 100 took 53.896s\n",
      "  training loss (in-iteration): \t2.269085\n",
      "  validation accuracy: \t\t\t32.21 %\n",
      "\t\tForward pass took  2.328 seconds\n",
      "\t\tBackward pass took 2.018 seconds\n",
      "Epoch 53 of 100 took 53.941s\n",
      "  training loss (in-iteration): \t2.268888\n",
      "  validation accuracy: \t\t\t32.25 %\n",
      "\t\tForward pass took  2.346 seconds\n",
      "\t\tBackward pass took 2.743 seconds\n",
      "Epoch 54 of 100 took 53.854s\n",
      "  training loss (in-iteration): \t2.266781\n",
      "  validation accuracy: \t\t\t31.97 %\n",
      "\t\tForward pass took  2.341 seconds\n",
      "\t\tBackward pass took 2.606 seconds\n",
      "Epoch 55 of 100 took 53.894s\n",
      "  training loss (in-iteration): \t2.266913\n",
      "  validation accuracy: \t\t\t32.01 %\n",
      "\t\tForward pass took  2.323 seconds\n",
      "\t\tBackward pass took 1.377 seconds\n",
      "Epoch 56 of 100 took 53.932s\n",
      "  training loss (in-iteration): \t2.255709\n",
      "  validation accuracy: \t\t\t32.11 %\n",
      "\t\tForward pass took  2.353 seconds\n",
      "\t\tBackward pass took 2.704 seconds\n",
      "Epoch 57 of 100 took 53.849s\n",
      "  training loss (in-iteration): \t2.262393\n",
      "  validation accuracy: \t\t\t32.11 %\n",
      "\t\tForward pass took  2.342 seconds\n",
      "\t\tBackward pass took 2.528 seconds\n",
      "Epoch 58 of 100 took 53.927s\n",
      "  training loss (in-iteration): \t2.264699\n",
      "  validation accuracy: \t\t\t32.27 %\n",
      "\t\tForward pass took  2.343 seconds\n",
      "\t\tBackward pass took 2.643 seconds\n",
      "Epoch 59 of 100 took 53.872s\n",
      "  training loss (in-iteration): \t2.271334\n",
      "  validation accuracy: \t\t\t31.72 %\n",
      "\t\tForward pass took  2.339 seconds\n",
      "\t\tBackward pass took 2.674 seconds\n",
      "Epoch 60 of 100 took 53.948s\n",
      "  training loss (in-iteration): \t2.272726\n",
      "  validation accuracy: \t\t\t31.33 %\n",
      "\t\tForward pass took  2.341 seconds\n",
      "\t\tBackward pass took 2.748 seconds\n",
      "Epoch 61 of 100 took 53.946s\n",
      "  training loss (in-iteration): \t2.268660\n",
      "  validation accuracy: \t\t\t32.12 %\n",
      "\t\tForward pass took  2.346 seconds\n",
      "\t\tBackward pass took 2.511 seconds\n",
      "Epoch 62 of 100 took 53.931s\n",
      "  training loss (in-iteration): \t2.268911\n",
      "  validation accuracy: \t\t\t32.04 %\n",
      "\t\tForward pass took  2.348 seconds\n",
      "\t\tBackward pass took 2.692 seconds\n",
      "Epoch 63 of 100 took 53.856s\n",
      "  training loss (in-iteration): \t2.248075\n",
      "  validation accuracy: \t\t\t31.87 %\n",
      "\t\tForward pass took  2.340 seconds\n",
      "\t\tBackward pass took 2.720 seconds\n",
      "Epoch 64 of 100 took 53.978s\n",
      "  training loss (in-iteration): \t2.257624\n",
      "  validation accuracy: \t\t\t31.99 %\n",
      "\t\tForward pass took  2.351 seconds\n",
      "\t\tBackward pass took 2.646 seconds\n",
      "Epoch 65 of 100 took 53.873s\n",
      "  training loss (in-iteration): \t2.268581\n",
      "  validation accuracy: \t\t\t31.71 %\n",
      "\t\tForward pass took  2.344 seconds\n",
      "\t\tBackward pass took 2.775 seconds\n",
      "Epoch 66 of 100 took 53.959s\n",
      "  training loss (in-iteration): \t2.253383\n",
      "  validation accuracy: \t\t\t31.62 %\n",
      "\t\tForward pass took  2.360 seconds\n",
      "\t\tBackward pass took 2.681 seconds\n",
      "Epoch 67 of 100 took 53.819s\n",
      "  training loss (in-iteration): \t2.268706\n",
      "  validation accuracy: \t\t\t31.74 %\n",
      "\t\tForward pass took  2.352 seconds\n",
      "\t\tBackward pass took 2.715 seconds\n",
      "Epoch 68 of 100 took 53.931s\n",
      "  training loss (in-iteration): \t2.255013\n",
      "  validation accuracy: \t\t\t31.53 %\n",
      "\t\tForward pass took  2.345 seconds\n",
      "\t\tBackward pass took 2.671 seconds\n",
      "Epoch 69 of 100 took 53.977s\n",
      "  training loss (in-iteration): \t2.264416\n",
      "  validation accuracy: \t\t\t31.65 %\n",
      "\t\tForward pass took  2.340 seconds\n",
      "\t\tBackward pass took 2.501 seconds\n",
      "Epoch 70 of 100 took 53.942s\n",
      "  training loss (in-iteration): \t2.264759\n",
      "  validation accuracy: \t\t\t31.76 %\n",
      "\t\tForward pass took  2.351 seconds\n",
      "\t\tBackward pass took 2.357 seconds\n",
      "Epoch 71 of 100 took 53.903s\n",
      "  training loss (in-iteration): \t2.252468\n",
      "  validation accuracy: \t\t\t31.93 %\n",
      "\t\tForward pass took  2.337 seconds\n",
      "\t\tBackward pass took 2.457 seconds\n",
      "Epoch 72 of 100 took 53.826s\n",
      "  training loss (in-iteration): \t2.249800\n",
      "  validation accuracy: \t\t\t31.34 %\n",
      "\t\tForward pass took  2.339 seconds\n",
      "\t\tBackward pass took 1.959 seconds\n",
      "Epoch 73 of 100 took 53.965s\n",
      "  training loss (in-iteration): \t2.257913\n",
      "  validation accuracy: \t\t\t31.58 %\n",
      "\t\tForward pass took  2.363 seconds\n",
      "\t\tBackward pass took 2.518 seconds\n",
      "Epoch 74 of 100 took 53.982s\n",
      "  training loss (in-iteration): \t2.252675\n",
      "  validation accuracy: \t\t\t31.98 %\n",
      "\t\tForward pass took  2.344 seconds\n",
      "\t\tBackward pass took 2.651 seconds\n",
      "Epoch 75 of 100 took 54.035s\n",
      "  training loss (in-iteration): \t2.240964\n",
      "  validation accuracy: \t\t\t31.90 %\n",
      "\t\tForward pass took  2.339 seconds\n",
      "\t\tBackward pass took 2.751 seconds\n",
      "Epoch 76 of 100 took 53.900s\n",
      "  training loss (in-iteration): \t2.234374\n",
      "  validation accuracy: \t\t\t32.16 %\n",
      "\t\tForward pass took  2.382 seconds\n",
      "\t\tBackward pass took 2.640 seconds\n",
      "Epoch 77 of 100 took 54.048s\n",
      "  training loss (in-iteration): \t2.246392\n",
      "  validation accuracy: \t\t\t31.39 %\n",
      "\t\tForward pass took  2.335 seconds\n",
      "\t\tBackward pass took 2.604 seconds\n",
      "Epoch 78 of 100 took 53.953s\n",
      "  training loss (in-iteration): \t2.249511\n",
      "  validation accuracy: \t\t\t31.46 %\n",
      "\t\tForward pass took  2.335 seconds\n",
      "\t\tBackward pass took 2.593 seconds\n",
      "Epoch 79 of 100 took 53.930s\n",
      "  training loss (in-iteration): \t2.271979\n",
      "  validation accuracy: \t\t\t31.73 %\n",
      "\t\tForward pass took  2.341 seconds\n",
      "\t\tBackward pass took 2.481 seconds\n",
      "Epoch 80 of 100 took 53.898s\n",
      "  training loss (in-iteration): \t2.255551\n",
      "  validation accuracy: \t\t\t31.67 %\n",
      "\t\tForward pass took  2.339 seconds\n",
      "\t\tBackward pass took 2.756 seconds\n",
      "Epoch 81 of 100 took 53.877s\n",
      "  training loss (in-iteration): \t2.261170\n",
      "  validation accuracy: \t\t\t31.37 %\n",
      "\t\tForward pass took  2.351 seconds\n",
      "\t\tBackward pass took 2.584 seconds\n",
      "Epoch 82 of 100 took 54.023s\n",
      "  training loss (in-iteration): \t2.241444\n",
      "  validation accuracy: \t\t\t31.79 %\n",
      "\t\tForward pass took  2.369 seconds\n",
      "\t\tBackward pass took 2.659 seconds\n",
      "Epoch 83 of 100 took 53.878s\n",
      "  training loss (in-iteration): \t2.249110\n",
      "  validation accuracy: \t\t\t31.54 %\n",
      "\t\tForward pass took  2.327 seconds\n",
      "\t\tBackward pass took 2.181 seconds\n",
      "Epoch 84 of 100 took 53.964s\n",
      "  training loss (in-iteration): \t2.235954\n",
      "  validation accuracy: \t\t\t31.73 %\n",
      "\t\tForward pass took  2.350 seconds\n",
      "\t\tBackward pass took 2.683 seconds\n",
      "Epoch 85 of 100 took 53.927s\n",
      "  training loss (in-iteration): \t2.236625\n",
      "  validation accuracy: \t\t\t31.70 %\n",
      "\t\tForward pass took  2.352 seconds\n",
      "\t\tBackward pass took 2.632 seconds\n",
      "Epoch 86 of 100 took 53.924s\n",
      "  training loss (in-iteration): \t2.239420\n",
      "  validation accuracy: \t\t\t31.47 %\n",
      "\t\tForward pass took  2.341 seconds\n",
      "\t\tBackward pass took 2.499 seconds\n",
      "Epoch 87 of 100 took 54.008s\n",
      "  training loss (in-iteration): \t2.239321\n",
      "  validation accuracy: \t\t\t31.82 %\n",
      "\t\tForward pass took  2.346 seconds\n",
      "\t\tBackward pass took 2.815 seconds\n",
      "Epoch 88 of 100 took 53.932s\n",
      "  training loss (in-iteration): \t2.243164\n",
      "  validation accuracy: \t\t\t31.37 %\n",
      "\t\tForward pass took  2.344 seconds\n",
      "\t\tBackward pass took 2.601 seconds\n",
      "Epoch 89 of 100 took 53.971s\n",
      "  training loss (in-iteration): \t2.242181\n",
      "  validation accuracy: \t\t\t31.59 %\n",
      "\t\tForward pass took  2.358 seconds\n",
      "\t\tBackward pass took 2.147 seconds\n",
      "Epoch 90 of 100 took 53.939s\n",
      "  training loss (in-iteration): \t2.236501\n",
      "  validation accuracy: \t\t\t31.57 %\n",
      "\t\tForward pass took  2.334 seconds\n",
      "\t\tBackward pass took 2.507 seconds\n",
      "Epoch 91 of 100 took 53.822s\n",
      "  training loss (in-iteration): \t2.234053\n",
      "  validation accuracy: \t\t\t31.49 %\n",
      "\t\tForward pass took  2.356 seconds\n",
      "\t\tBackward pass took 2.408 seconds\n",
      "Epoch 92 of 100 took 53.826s\n",
      "  training loss (in-iteration): \t2.224098\n",
      "  validation accuracy: \t\t\t31.39 %\n",
      "\t\tForward pass took  2.343 seconds\n",
      "\t\tBackward pass took 2.615 seconds\n",
      "Epoch 93 of 100 took 53.901s\n",
      "  training loss (in-iteration): \t2.256032\n",
      "  validation accuracy: \t\t\t31.36 %\n",
      "\t\tForward pass took  2.375 seconds\n",
      "\t\tBackward pass took 2.765 seconds\n",
      "Epoch 94 of 100 took 53.895s\n",
      "  training loss (in-iteration): \t2.239753\n",
      "  validation accuracy: \t\t\t31.09 %\n",
      "\t\tForward pass took  2.355 seconds\n",
      "\t\tBackward pass took 2.692 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-f7162c6dde61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                       \u001b[0muse_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                       effective_batch_size=1024)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-1d22a7b6b0fa>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(student_model, teacher_model, opt, loss_fn, model_checkpoint_path, data_path, use_checkpoint, gpu, batch_size, epochs, effective_batch_size)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mstart_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                 \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_checkpoint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "student_model = train(student_model=student_model, teacher_model=teacher_model, \n",
    "                      opt=opt, loss_fn=loss_fn, \n",
    "                      model_checkpoint_path='student_checkpoints', \n",
    "                      data_path='tiny-imagenet-200', \n",
    "                      gpu=gpu_enabled, \n",
    "                      batch_size=128, \n",
    "                      epochs=100, \n",
    "                      use_checkpoint=False, \n",
    "                      effective_batch_size=1024)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t30.97 %\n"
     ]
    }
   ],
   "source": [
    "validate(student_model, data_path='tiny-imagenet-200', batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На исходных данных эта сеть за аналогичное число шагов обучилась до 25.44 %, что говорит о практической пользе подхода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning\n",
    "\n",
    "The idea of pruning is to remove unnecessary (in terms of loss) weights. It can be measured in different ways: for example, by the norm of the weights (similar to L1 feature selection), by the magnitude of the activation or via Taylor expansion*.\n",
    "\n",
    "One iteration of pruning consists of two steps:\n",
    "\n",
    "1) Rank weights with some importance measure and remove the least important\n",
    "\n",
    "2) Fine-tune the model\n",
    "\n",
    "This approach is a bit computationally heavy but can lead to drastic (up to 150x) decrease of memory to store the weights. Moreover if you make use of structure in layers you can decrease also compute. For example, the whole convolutional filters can be removed.\n",
    "\n",
    "*https://arxiv.org/pdf/1611.06440.pdf"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "homework_optimization.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
