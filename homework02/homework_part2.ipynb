{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Homework 2.2: The Quest For A Better Network\n",
    "\n",
    "In this assignment you will build a monster network to solve Tiny ImageNet image classification.\n",
    "\n",
    "This notebook is intended as a sequel to seminar 3, please give it a try if you haven't done so yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(please read it at least diagonally)\n",
    "\n",
    "* The ultimate quest is to create a network that has as high __accuracy__ as you can push it.\n",
    "* There is a __mini-report__ at the end that you will have to fill in. We recommend reading it first and filling it while you iterate.\n",
    " \n",
    "## Grading\n",
    "* starting at zero points\n",
    "* +20% for describing your iteration path in a report below.\n",
    "* +20% for building a network that gets above 20% accuracy\n",
    "* +10% for beating each of these milestones on __TEST__ dataset:\n",
    "    * 25% (50% points)\n",
    "    * 30% (60% points)\n",
    "    * 32.5% (70% points)\n",
    "    * 35% (80% points)\n",
    "    * 37.5% (90% points)\n",
    "    * 40% (full points)\n",
    "    \n",
    "## Restrictions\n",
    "* Please do NOT use pre-trained networks for this assignment until you reach 40%.\n",
    " * In other words, base milestones must be beaten without pre-trained nets (and such net must be present in the anytask atttachments). After that, you can use whatever you want.\n",
    "* you __can't__ do anything with validation data apart from running the evaluation procedure. Please, split train images on train and validation parts\n",
    "\n",
    "## Tips on what can be done:\n",
    "\n",
    "\n",
    " * __Network size__\n",
    "   * MOAR neurons, \n",
    "   * MOAR layers, ([torch.nn docs](http://pytorch.org/docs/master/nn.html))\n",
    "\n",
    "   * Nonlinearities in the hidden layers\n",
    "     * tanh, relu, leaky relu, etc\n",
    "   * Larger networks may take more epochs to train, so don't discard your net just because it could didn't beat the baseline in 5 epochs.\n",
    "\n",
    "   * Ph'nglui mglw'nafh Cthulhu R'lyeh wgah'nagl fhtagn!\n",
    "\n",
    "\n",
    "### The main rule of prototyping: one change at a time\n",
    "   * By now you probably have several ideas on what to change. By all means, try them out! But there's a catch: __never test several new things at once__.\n",
    "\n",
    "\n",
    "### Optimization\n",
    "   * Training for 100 epochs regardless of anything is probably a bad idea.\n",
    "   * Some networks converge over 5 epochs, others - over 500.\n",
    "   * Way to go: stop when validation score is 10 iterations past maximum\n",
    "   * You should certainly use adaptive optimizers\n",
    "     * rmsprop, nesterov_momentum, adam, adagrad and so on.\n",
    "     * Converge faster and sometimes reach better optima\n",
    "     * It might make sense to tweak learning rate/momentum, other learning parameters, batch size and number of epochs\n",
    "   * __BatchNormalization__ (nn.BatchNorm2d) for the win!\n",
    "     * Sometimes more batch normalization is better.\n",
    "   * __Regularize__ to prevent overfitting\n",
    "     * Add some L2 weight norm to the loss function, PyTorch will do the rest\n",
    "       * Can be done manually or like [this](https://discuss.pytorch.org/t/simple-l2-regularization/139/2).\n",
    "     * Dropout (`nn.Dropout`) - to prevent overfitting\n",
    "       * Don't overdo it. Check if it actually makes your network better\n",
    "   \n",
    "### Convolution architectures\n",
    "   * This task __can__ be solved by a sequence of convolutions and poolings with batch_norm and ReLU seasoning, but you shouldn't necessarily stop there.\n",
    "   * [Inception family](https://hacktilldawn.com/2016/09/25/inception-modules-explained-and-implemented/), [ResNet family](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035?gi=9018057983ca), [Densely-connected convolutions (exotic)](https://arxiv.org/abs/1608.06993), [Capsule networks (exotic)](https://arxiv.org/abs/1710.09829)\n",
    "   * Please do try a few simple architectures before you go for resnet-152.\n",
    "   * Warning! Training convolutional networks can take long without GPU. That's okay.\n",
    "     * If you are CPU-only, we still recomment that you try a simple convolutional architecture\n",
    "     * a perfect option is if you can set it up to run at nighttime and check it up at the morning.\n",
    "     * Make reasonable layer size estimates. A 128-neuron first convolution is likely an overkill.\n",
    "     * __To reduce computation__ time by a factor in exchange for some accuracy drop, try using __stride__ parameter. A stride=2 convolution should take roughly 1/4 of the default (stride=1) one.\n",
    " \n",
    "   \n",
    "### Data augmemntation\n",
    "   * getting 5x as large dataset for free is a great \n",
    "     * Zoom-in+slice = move\n",
    "     * Rotate+zoom(to remove black stripes)\n",
    "     * Add Noize (gaussian or bernoulli)\n",
    "   * Simple way to do that (if you have PIL/Image): \n",
    "     * ```from scipy.misc import imrotate,imresize```\n",
    "     * and a few slicing\n",
    "     * Other cool libraries: cv2, skimake, PIL/Pillow\n",
    "   * A more advanced way is to use torchvision transforms:\n",
    "    ```\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    trainset = torchvision.datasets.ImageFolder(root=path_to_tiny_imagenet, train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "    ```\n",
    "   * Or use this tool from Keras (requires theano/tensorflow): [tutorial](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), [docs](https://keras.io/preprocessing/image/)\n",
    "   * Stay realistic. There's usually no point in flipping dogs upside down as that is not the way you usually see them.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./tiny-imagenet-200.zip\n"
     ]
    }
   ],
   "source": [
    "# from tiny_img import download_tinyImg200\n",
    "# data_path = '.'\n",
    "# download_tinyImg200(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "means = np.array([0.485, 0.456, 0.406])\n",
    "stds = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation((-30,30)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(means, stds)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(means, stds)\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(means, stds)\n",
    "    ])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.ImageFolder('tiny-imagenet-200/train', transform=transforms['train'])\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [80000, 20000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_batch_gen = torch.utils.data.DataLoader(train_dataset, \n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=4)\n",
    "val_batch_gen = torch.utils.data.DataLoader(val_dataset, \n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torchvision.datasets.ImageFolder('tiny-imagenet-200/new_val/', transform=transforms['val'])\n",
    "test_batch_gen = torch.utils.data.DataLoader(test_dataset, \n",
    "                                             batch_size=batch_size,\n",
    "                                             shuffle=False,\n",
    "                                             num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# a special module that converts [batch, channel, w, h] to [batch, units]\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15477778889165552190"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "torch.initial_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = [64, 'M', 128, 'M', 256, 256, 'M'] \n",
    "\n",
    "in_channels = 3\n",
    "layers = []\n",
    "for v in cfg:\n",
    "    if v == 'M':\n",
    "        layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "    else:\n",
    "        conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "        layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "        in_channels = v\n",
    "layers += [\n",
    "    Flatten(),\n",
    "    nn.Linear(16384, 1024),\n",
    "    nn.ReLU(True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.ReLU(True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(512, 200),\n",
    "]\n",
    "model = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "# opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loss = []\n",
    "val_accuracy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 1000 took 45.841s\n",
      "  training loss (in-iteration): \t3.700233\n",
      "  validation accuracy: \t\t\t21.98 %\n",
      "Epoch 2 of 1000 took 46.259s\n",
      "  training loss (in-iteration): \t3.612604\n",
      "  validation accuracy: \t\t\t23.06 %\n",
      "Epoch 3 of 1000 took 47.330s\n",
      "  training loss (in-iteration): \t3.539748\n",
      "  validation accuracy: \t\t\t21.51 %\n",
      "Epoch 4 of 1000 took 47.739s\n",
      "  training loss (in-iteration): \t3.467154\n",
      "  validation accuracy: \t\t\t23.07 %\n",
      "Epoch 5 of 1000 took 47.959s\n",
      "  training loss (in-iteration): \t3.395841\n",
      "  validation accuracy: \t\t\t26.11 %\n",
      "Epoch 6 of 1000 took 47.896s\n",
      "  training loss (in-iteration): \t3.336965\n",
      "  validation accuracy: \t\t\t27.47 %\n",
      "Epoch 7 of 1000 took 47.949s\n",
      "  training loss (in-iteration): \t3.264905\n",
      "  validation accuracy: \t\t\t29.08 %\n",
      "Epoch 8 of 1000 took 47.940s\n",
      "  training loss (in-iteration): \t3.215509\n",
      "  validation accuracy: \t\t\t28.55 %\n",
      "Epoch 9 of 1000 took 47.901s\n",
      "  training loss (in-iteration): \t3.159193\n",
      "  validation accuracy: \t\t\t29.05 %\n",
      "Epoch 10 of 1000 took 47.991s\n",
      "  training loss (in-iteration): \t3.101248\n",
      "  validation accuracy: \t\t\t31.05 %\n",
      "Epoch 11 of 1000 took 47.958s\n",
      "  training loss (in-iteration): \t3.065050\n",
      "  validation accuracy: \t\t\t33.06 %\n",
      "Epoch 12 of 1000 took 47.897s\n",
      "  training loss (in-iteration): \t3.022019\n",
      "  validation accuracy: \t\t\t31.25 %\n",
      "Epoch 13 of 1000 took 47.877s\n",
      "  training loss (in-iteration): \t2.986795\n",
      "  validation accuracy: \t\t\t33.22 %\n",
      "Epoch 14 of 1000 took 47.858s\n",
      "  training loss (in-iteration): \t2.951682\n",
      "  validation accuracy: \t\t\t33.05 %\n",
      "Epoch 15 of 1000 took 47.940s\n",
      "  training loss (in-iteration): \t2.913631\n",
      "  validation accuracy: \t\t\t34.09 %\n",
      "Epoch 16 of 1000 took 47.886s\n",
      "  training loss (in-iteration): \t2.880684\n",
      "  validation accuracy: \t\t\t34.87 %\n",
      "Epoch 17 of 1000 took 47.959s\n",
      "  training loss (in-iteration): \t2.855452\n",
      "  validation accuracy: \t\t\t34.25 %\n",
      "Epoch 18 of 1000 took 47.960s\n",
      "  training loss (in-iteration): \t2.828766\n",
      "  validation accuracy: \t\t\t34.96 %\n",
      "Epoch 19 of 1000 took 47.926s\n",
      "  training loss (in-iteration): \t2.803041\n",
      "  validation accuracy: \t\t\t35.31 %\n",
      "Epoch 20 of 1000 took 47.932s\n",
      "  training loss (in-iteration): \t2.777298\n",
      "  validation accuracy: \t\t\t34.79 %\n",
      "Epoch 21 of 1000 took 47.908s\n",
      "  training loss (in-iteration): \t2.751525\n",
      "  validation accuracy: \t\t\t35.99 %\n",
      "Epoch 22 of 1000 took 47.950s\n",
      "  training loss (in-iteration): \t2.728628\n",
      "  validation accuracy: \t\t\t36.16 %\n",
      "Epoch 23 of 1000 took 47.918s\n",
      "  training loss (in-iteration): \t2.701825\n",
      "  validation accuracy: \t\t\t36.96 %\n",
      "Epoch 24 of 1000 took 47.977s\n",
      "  training loss (in-iteration): \t2.689486\n",
      "  validation accuracy: \t\t\t36.14 %\n",
      "Epoch 25 of 1000 took 47.956s\n",
      "  training loss (in-iteration): \t2.672224\n",
      "  validation accuracy: \t\t\t35.53 %\n",
      "Epoch 26 of 1000 took 48.027s\n",
      "  training loss (in-iteration): \t2.653154\n",
      "  validation accuracy: \t\t\t36.70 %\n",
      "Epoch 27 of 1000 took 48.037s\n",
      "  training loss (in-iteration): \t2.630744\n",
      "  validation accuracy: \t\t\t37.06 %\n",
      "Epoch 28 of 1000 took 47.982s\n",
      "  training loss (in-iteration): \t2.620001\n",
      "  validation accuracy: \t\t\t37.09 %\n",
      "Epoch 29 of 1000 took 47.973s\n",
      "  training loss (in-iteration): \t2.604057\n",
      "  validation accuracy: \t\t\t37.85 %\n",
      "Epoch 30 of 1000 took 47.903s\n",
      "  training loss (in-iteration): \t2.589370\n",
      "  validation accuracy: \t\t\t36.19 %\n",
      "Epoch 31 of 1000 took 47.914s\n",
      "  training loss (in-iteration): \t2.573342\n",
      "  validation accuracy: \t\t\t36.49 %\n",
      "Epoch 32 of 1000 took 47.963s\n",
      "  training loss (in-iteration): \t2.556373\n",
      "  validation accuracy: \t\t\t38.19 %\n",
      "Epoch 33 of 1000 took 47.991s\n",
      "  training loss (in-iteration): \t2.544831\n",
      "  validation accuracy: \t\t\t36.63 %\n",
      "Epoch 34 of 1000 took 47.974s\n",
      "  training loss (in-iteration): \t2.526521\n",
      "  validation accuracy: \t\t\t37.83 %\n",
      "Epoch 35 of 1000 took 47.987s\n",
      "  training loss (in-iteration): \t2.512908\n",
      "  validation accuracy: \t\t\t37.93 %\n",
      "Epoch 38 of 1000 took 48.004s\n",
      "  training loss (in-iteration): \t2.479757\n",
      "  validation accuracy: \t\t\t38.58 %\n",
      "Epoch 39 of 1000 took 47.884s\n",
      "  training loss (in-iteration): \t2.472788\n",
      "  validation accuracy: \t\t\t37.63 %\n",
      "Epoch 40 of 1000 took 47.927s\n",
      "  training loss (in-iteration): \t2.469804\n",
      "  validation accuracy: \t\t\t38.33 %\n",
      "Epoch 41 of 1000 took 47.957s\n",
      "  training loss (in-iteration): \t2.452011\n",
      "  validation accuracy: \t\t\t39.49 %\n",
      "Epoch 42 of 1000 took 47.958s\n",
      "  training loss (in-iteration): \t2.435422\n",
      "  validation accuracy: \t\t\t38.54 %\n",
      "Epoch 43 of 1000 took 47.985s\n",
      "  training loss (in-iteration): \t2.432925\n",
      "  validation accuracy: \t\t\t39.75 %\n",
      "Epoch 44 of 1000 took 47.847s\n",
      "  training loss (in-iteration): \t2.418007\n",
      "  validation accuracy: \t\t\t39.31 %\n",
      "Epoch 45 of 1000 took 47.945s\n",
      "  training loss (in-iteration): \t2.419508\n",
      "  validation accuracy: \t\t\t39.58 %\n",
      "Epoch 46 of 1000 took 48.023s\n",
      "  training loss (in-iteration): \t2.409874\n",
      "  validation accuracy: \t\t\t39.54 %\n",
      "Epoch 47 of 1000 took 48.021s\n",
      "  training loss (in-iteration): \t2.400872\n",
      "  validation accuracy: \t\t\t37.47 %\n",
      "Epoch 48 of 1000 took 47.946s\n",
      "  training loss (in-iteration): \t2.398879\n",
      "  validation accuracy: \t\t\t40.05 %\n",
      "Epoch 49 of 1000 took 47.955s\n",
      "  training loss (in-iteration): \t2.380504\n",
      "  validation accuracy: \t\t\t40.24 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-0b532fb956a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mval_accuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Then we print the results for this epoch:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_epochs = 1000 # total amount of full passes over training data\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    start_time = time.time()\n",
    "    model.train(True) # enable dropout / batch_norm training behavior\n",
    "    for (X_batch, y_batch) in train_batch_gen:\n",
    "        # train on batch\n",
    "        X_batch = Variable(torch.FloatTensor(X_batch)).cuda()\n",
    "        y_batch = Variable(torch.LongTensor(y_batch)).cuda()\n",
    "        logits = model.cuda()(X_batch)\n",
    "        loss = loss_fn(logits, y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        train_loss.append(loss.data.cpu().numpy())\n",
    "    \n",
    "    model.train(False) # disable dropout / use averages for batch_norm\n",
    "    for X_batch, y_batch in val_batch_gen:\n",
    "        logits = model(Variable(torch.FloatTensor(X_batch)).cuda())\n",
    "        y_pred = logits.max(1)[1].data\n",
    "        val_accuracy.append(np.mean( (y_batch.cpu() == y_pred.cpu()).numpy() ))\n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "        np.mean(train_loss[-len(train_dataset) // batch_size :])))\n",
    "    print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
    "        np.mean(val_accuracy[-len(val_dataset) // batch_size :]) * 100))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(4,4)\n",
    "# for i in range(4):\n",
    "#     for j in range(4):\n",
    "#         axes[i, j].imshow(X_batch[4*i+j].cpu().numpy().transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 394.,  402.,  403.,  399.,  412.,  402.,  400.,  397.,  404.,\n",
       "        401.,  394.,  395.,  386.,  404.,  400.,  402.,  406.,  409.,\n",
       "        407.,  403.,  399.,  410.,  407.,  395.,  397.,  396.,  397.,\n",
       "        405.,  400.,  411.,  400.,  393.,  394.,  393.,  383.,  404.,\n",
       "        392.,  395.,  407.,  391.,  397.,  396.,  410.,  421.,  419.,\n",
       "        394.,  397.,  403.,  417.,  393.,  401.,  389.,  405.,  393.,\n",
       "        399.,  395.,  405.,  417.,  396.,  384.,  392.,  418.,  408.,\n",
       "        405.,  404.,  404.,  396.,  404.,  404.,  407.,  407.,  400.,\n",
       "        383.,  410.,  406.,  391.,  408.,  409.,  383.,  400.,  399.,\n",
       "        399.,  402.,  417.,  410.,  398.,  417.,  392.,  405.,  394.,\n",
       "        410.,  389.,  402.,  408.,  406.,  393.,  402.,  394.,  396.,\n",
       "        406.,  415.,  420.,  406.,  406.,  395.,  401.,  407.,  409.,\n",
       "        401.,  386.,  414.,  402.,  399.,  409.,  393.,  388.,  412.,\n",
       "        385.,  412.,  392.,  396.,  404.,  381.,  411.,  409.,  390.,\n",
       "        393.,  397.,  397.,  402.,  381.,  413.,  397.,  399.,  402.,\n",
       "        402.,  388.,  396.,  403.,  413.,  399.,  412.,  400.,  408.,\n",
       "        407.,  376.,  408.,  410.,  402.,  394.,  414.,  386.,  398.,\n",
       "        388.,  390.,  399.,  407.,  403.,  402.,  403.,  396.,  385.,\n",
       "        408.,  398.,  393.,  383.,  389.,  392.,  408.,  401.,  398.,\n",
       "        396.,  402.,  392.,  393.,  425.,  412.,  391.,  413.,  400.,\n",
       "        392.,  395.,  392.,  410.,  392.,  393.,  387.,  394.,  385.,\n",
       "        416.,  400.,  398.,  376.,  381.,  392.,  383.,  407.,  401.,\n",
       "        402.,  399.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.zeros((200))\n",
    "\n",
    "for (X_batch, y_batch) in train_batch_gen:\n",
    "    y_train = y_train + np.sum(np.eye(200)[y_batch.numpy()], axis=0)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,\n",
       "        50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,\n",
       "        50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,\n",
       "        50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,\n",
       "        50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,\n",
       "        50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,\n",
       "        50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,\n",
       "        50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,\n",
       "        50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,\n",
       "        50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,\n",
       "        50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,\n",
       "        50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,\n",
       "        50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,\n",
       "        50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,\n",
       "        50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,\n",
       "        50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,\n",
       "        50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,\n",
       "        50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,  50.,\n",
       "        50.,  50.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = np.zeros((200))\n",
    "\n",
    "for (X_batch, y_batch) in test_batch_gen:\n",
    "    y_test = y_test + np.sum(np.eye(200)[y_batch.numpy()], axis=0)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(4,4)\n",
    "# for i in range(4):\n",
    "#     for j in range(4):\n",
    "#         axes[i, j].imshow(X_batch[4*i+j].cpu().numpy().transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When everything is done, please calculate accuracy on `tiny-imagenet-200/val`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(False) # disable dropout / use averages for batch_norm\n",
    "train_acc = []\n",
    "\n",
    "for X_batch, y_batch in train_batch_gen:\n",
    "    logits = model(Variable(torch.FloatTensor(X_batch)).cuda())\n",
    "    y_pred = logits.max(1)[1].data\n",
    "    train_acc += list((y_batch.cpu() == y_pred.cpu()).numpy())\n",
    "\n",
    "train_accuracy = np.mean(train_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53382499999999999"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(False) # disable dropout / use averages for batch_norm\n",
    "test_acc = []\n",
    "\n",
    "for X_batch, y_batch in test_batch_gen:\n",
    "    logits = model(Variable(torch.FloatTensor(X_batch)).cuda())\n",
    "    y_pred = logits.max(1)[1].data\n",
    "    test_acc += list((y_batch.cpu() == y_pred.cpu()).numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = np.mean(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results:\n",
      "  test accuracy:\t\t41.71 %\n",
      "Achievement unlocked: 80lvl Warlock!\n"
     ]
    }
   ],
   "source": [
    "print(\"Final results:\")\n",
    "print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "    test_accuracy * 100))\n",
    "\n",
    "if test_accuracy * 100 > 70:\n",
    "    print(\"U'r freakin' amazin'!\")\n",
    "elif test_accuracy * 100 > 50:\n",
    "    print(\"Achievement unlocked: 110lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 40:\n",
    "    print(\"Achievement unlocked: 80lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 30:\n",
    "    print(\"Achievement unlocked: 70lvl Warlock!\")\n",
    "elif test_accuracy * 100 > 20:\n",
    "    print(\"Achievement unlocked: 60lvl Warlock!\")\n",
    "else:\n",
    "    print(\"We need more magic! Follow instructons below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir model_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\t\t homework_part1.ipynb  tiny-imagenet-200\r\n",
      "Untitled.ipynb\t\t homework_part2.ipynb  tiny-imagenet-200.zip\r\n",
      "__pycache__\t\t model_checkpoints     tiny_img.py\r\n",
      "homework_advanced.ipynb  notmnist.py\t       tiny_img.pyc\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_state_dict_41.71.pcl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отчет\n",
    "\n",
    "Меня зовут Руденко Ирина и в рамках своих рабочих обязанностей я занимаюсь приложениями нейросетевой сегментации на keras, а это мой первый опыт с pytorch\n",
    "\n",
    "Вот моя иcтория:\n",
    "* я написала стандартную блочную архитектуру, (conv+batchnorm+relu) + иногда pooling, после flatten добавила пару скрытых слоёв с дропаутом, в качестве активации также взяла relu, а последней - softmax\n",
    "* беда! на валидации ~30%, а на тесте ~0.5%\n",
    "* прорешала семинар, поняла, что softmax с данной функцией потерь - лишний, так что я убрала его и попробовала снова.\n",
    "* решила, что sgd слишком тупой, сделала Adam - не помогло 0.47 %\n",
    "* посмотрела косым взглядом на тест, посмотрела на формирование теста в семинаре(там и тест и валидация - куски трейна)\n",
    "* проверила распределение классов на трейне и тесте, весь тест был одного класса, а вот картинки это не подтверждали, и простая сетка с семинара также давала почти нулевое качество на правильном тесте\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "def ensure_dir(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "        \n",
    "with open('tiny-imagenet-200/val/val_annotations.txt') as f:\n",
    "    for s in f.readlines():\n",
    "        tokens = s.split()\n",
    "        ensure_dir('tiny-imagenet-200/new_val/%s/images/'% tokens[1])\n",
    "        shutil.copy2(os.path.join('tiny-imagenet-200/val/images/', tokens[0]), \n",
    "                     os.path.join('tiny-imagenet-200/new_val/%s/images/' % tokens[1], tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\uparrow$ Вот этот скрипт перекладывает тест правильным образом\n",
    "\n",
    "После этого я просто запустила исходную сетку без софтмакса и поучила полчасика на gpu\n",
    "\n",
    "Итог:\n",
    "\n",
    "|   | acc, %  |\n",
    "|---|---|\n",
    "| train  | 53.38  |\n",
    "| val    | 40.24  |\n",
    "| test   | 41.71  |\n",
    "\n",
    "\n",
    "По итогам заняло 3 вечера, причём баг искала 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "# Report\n",
    "\n",
    "All creative approaches are highly welcome, but at the very least it would be great to mention\n",
    "* the idea;\n",
    "* brief history of tweaks and improvements;\n",
    "* what is the final architecture and why?\n",
    "* what is the training method and, again, why?\n",
    "* Any regularizations and other techniques applied and their effects;\n",
    "\n",
    "\n",
    "There is no need to write strict mathematical proofs (unless you want to).\n",
    " * \"I tried this, this and this, and the second one turned out to be better. And i just didn't like the name of that one\" - OK, but can be better\n",
    " * \"I have analized these and these articles|sources|blog posts, tried that and that to adapt them to my problem and the conclusions are such and such\" - the ideal one\n",
    " * \"I took that code that demo without understanding it, but i'll never confess that and instead i'll make up some pseudoscientific explaination\" - __not_ok__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
